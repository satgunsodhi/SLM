{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"1b59cf1959a443aebc4cac7a12408f40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f168741e8864ad9a5576f3a4ad36481":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28acae29bf0e41768a2d8b48f7fc3208":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"632924c86b054ac58184edbc0a61d2f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28acae29bf0e41768a2d8b48f7fc3208","placeholder":"​","style":"IPY_MODEL_ef82bd584ed24f1bbb62e19d13ea705a","value":" 326716/2119719 [01:03&lt;04:40, 6387.55 examples/s]"}},"7a31a5e397c748cc9f61f2e071e2d9d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d780907ca5094fb5b48b770aaccc9052","IPY_MODEL_b04b4bf523374b50b4784b8f0205330d","IPY_MODEL_632924c86b054ac58184edbc0a61d2f2"],"layout":"IPY_MODEL_1b59cf1959a443aebc4cac7a12408f40"}},"92fc8390416e41f38673c6666fcce44c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9d653658ad2b426db4582b56f811c6aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b04b4bf523374b50b4784b8f0205330d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f168741e8864ad9a5576f3a4ad36481","max":2119719,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92fc8390416e41f38673c6666fcce44c","value":327514}},"d780907ca5094fb5b48b770aaccc9052":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d653658ad2b426db4582b56f811c6aa","placeholder":"​","style":"IPY_MODEL_e585f202ec8e4b3bb288ae89f254d15b","value":"tokenizing the splits (num_proc=2):  15%"}},"e585f202ec8e4b3bb288ae89f254d15b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef82bd584ed24f1bbb62e19d13ea705a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Step 1: Prerequisites","metadata":{}},{"cell_type":"code","source":"%pip install datasets torch torchvision tokenizer tiktoken matplotlib ipywidgets jupyter transformers","metadata":{"id":"DoI7cICAeKAY","outputId":"da037767-0e41-4efb-da3e-14f27db5c329","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:48:35.785890Z","iopub.execute_input":"2025-10-30T13:48:35.786091Z","iopub.status.idle":"2025-10-30T13:49:56.300551Z","shell.execute_reply.started":"2025-10-30T13:48:35.786068Z","shell.execute_reply":"2025-10-30T13:49:56.299669Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.1.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nCollecting tokenizer\n  Downloading tokenizer-3.5.3-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (8.1.5)\nCollecting jupyter\n  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.19.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.0.0rc2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2025.9.18)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.3)\nRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\nRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (4.0.14)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\nRequirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter) (6.5.4)\nRequirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter) (6.1.0)\nRequirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter) (6.4.5)\nRequirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter) (6.17.1)\nRequirement already satisfied: jupyterlab in /usr/local/lib/python3.11/dist-packages (from jupyter) (3.6.8)\nCollecting huggingface-hub>=0.24.0 (from datasets)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (75.2.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\nRequirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\nRequirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (1.8.15)\nRequirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (8.6.3)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (1.6.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (7.1.0)\nRequirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (26.2.1)\nRequirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (6.5.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (5.8.1)\nRequirement already satisfied: jupyterlab-server~=2.19 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (2.27.3)\nRequirement already satisfied: jupyter-server<3,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (2.12.5)\nRequirement already satisfied: jupyter-ydoc~=0.2.4 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (0.2.5)\nRequirement already satisfied: jupyter-server-ydoc~=0.8.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (0.8.0)\nRequirement already satisfied: nbclassic in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (1.3.1)\nRequirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (25.1.0)\nRequirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (0.2.0)\nRequirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (5.10.4)\nRequirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (1.8.3)\nRequirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (0.18.1)\nRequirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (0.22.1)\nRequirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.3.0)\nRequirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.4)\nRequirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (6.2.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (1.5.1)\nRequirement already satisfied: testpath in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.6.0)\nRequirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (4.13.4)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.5.13)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\nRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab->jupyter) (4.4.0)\nRequirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (4.11.0)\nRequirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (0.12.0)\nRequirement already satisfied: jupyter-server-terminals in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (0.5.3)\nRequirement already satisfied: overrides in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (7.7.0)\nRequirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (1.8.0)\nRequirement already satisfied: jupyter-server-fileid<1,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter) (0.9.3)\nRequirement already satisfied: ypy-websocket<0.9.0,>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter) (0.8.4)\nRequirement already satisfied: y-py<0.7.0,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-ydoc~=0.2.4->jupyterlab->jupyter) (0.6.2)\nRequirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server~=2.19->jupyterlab->jupyter) (2.17.0)\nRequirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server~=2.19->jupyterlab->jupyter) (0.12.1)\nRequirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server~=2.19->jupyterlab->jupyter) (4.25.0)\nRequirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic->jupyterlab->jupyter) (0.2.4)\nRequirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook->jupyter) (2.21.1)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter) (2.7)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->nbconvert->jupyter) (0.5.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (1.3.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server~=2.19->jupyterlab->jupyter) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server~=2.19->jupyterlab->jupyter) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server~=2.19->jupyterlab->jupyter) (0.26.0)\nRequirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (3.3.0)\nRequirement already satisfied: rfc3339-validator in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (0.1.1)\nRequirement already satisfied: aiofiles<23,>=22.1.0 in /usr/local/lib/python3.11/dist-packages (from ypy-websocket<0.9.0,>=0.8.2->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter) (22.1.0)\nRequirement already satisfied: aiosqlite<1,>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from ypy-websocket<0.9.0,>=0.8.2->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter) (0.21.0)\nRequirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.23)\nRequirement already satisfied: fqdn in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (1.5.1)\nRequirement already satisfied: isoduration in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (3.0.0)\nRequirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (1.1.0)\nRequirement already satisfied: uri-template in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (1.3.0)\nRequirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (24.11.1)\nRequirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (1.3.0)\nRequirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (1.3.0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (2.9.0.20250822)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizer-3.5.3-py3-none-any.whl (81 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\nDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizer, pyarrow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12, jupyter\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 jupyter-1.1.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyarrow-22.0.0 tokenizer-3.5.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\n\n# Check available GPUs\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:49:56.302360Z","iopub.execute_input":"2025-10-30T13:49:56.302579Z","iopub.status.idle":"2025-10-30T13:49:59.113509Z","shell.execute_reply.started":"2025-10-30T13:49:56.302561Z","shell.execute_reply":"2025-10-30T13:49:59.112857Z"}},"outputs":[{"name":"stdout","text":"Number of GPUs: 2\n  GPU 0: Tesla T4\n  GPU 1: Tesla T4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Step 2: Load Multiple Financial Datasets","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets, DatasetDict, Dataset\nimport tiktoken\nimport os\nimport numpy as np\nfrom tqdm.auto import tqdm\n\n\nprint(\"=\" * 80)\nprint(\"FINANCIAL SLM - Multi-Dataset Preparation\")\nprint(\"=\" * 80)\n\n\n# DATASET 1: Finance Alpaca (Instruction-following Q&A)\nprint(\"\\n[1/4] Loading Finance Alpaca dataset...\")\nfinance_alpaca = load_dataset(\"gbharti/finance-alpaca\")\nprint(f\"✓ Finance Alpaca loaded: {len(finance_alpaca['train']):,} examples\")\nprint(f\"  Purpose: Financial instruction-following and Q&A\")\n\n\n# DATASET 2: Financial PhraseBank (Sentiment Analysis)\nprint(\"\\n[2/4] Loading Financial PhraseBank dataset...\")\nfinancial_phrasebank = load_dataset(\"atrost/financial_phrasebank\")\nprint(f\"✓ Financial PhraseBank loaded: {len(financial_phrasebank['train']):,} examples\")\nprint(f\"  Purpose: Financial sentiment understanding (positive/negative/neutral)\")\n\n\nsentiment_counts = {}\nfor example in financial_phrasebank['train']:\n    label = example['label']\n    sentiment_counts[label] = sentiment_counts.get(label, 0) + 1\nprint(f\"  Sentiment distribution: {sentiment_counts}\")\n\n\n# DATASET 3: General English Alpaca (Basic English Understanding)\nprint(\"\\n[3/4] Loading General Alpaca dataset for English understanding...\")\ntry:\n    general_alpaca = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n    # Sample 20,000 examples for balanced mix (adjust based on your needs)\n    general_alpaca_sample = general_alpaca.shuffle(seed=42).select(range(min(20000, len(general_alpaca))))\n    print(f\"✓ General Alpaca loaded: {len(general_alpaca_sample):,} examples\")\n    print(f\"  Purpose: General English instruction-following and reasoning\")\nexcept Exception as e:\n    print(f\"⚠ Failed to load general Alpaca dataset: {e}\")\n    print(f\"  Using fallback: Creating minimal general English examples\")\n    # Fallback: create small general English dataset\n    general_alpaca_sample = Dataset.from_list([\n        {\"instruction\": \"What is 2+2?\", \"input\": \"\", \"output\": \"2+2 equals 4. This is a basic arithmetic operation where we add two and two together.\"},\n        {\"instruction\": \"Explain photosynthesis\", \"input\": \"\", \"output\": \"Photosynthesis is the process plants use to convert sunlight into chemical energy. Plants absorb carbon dioxide and water, then use light energy to produce glucose and oxygen.\"}\n    ])\n    print(f\"✓ Fallback dataset created: {len(general_alpaca_sample)} examples\")\n\n\n# DATASET 4: Custom Investment Dataset\nprint(\"\\n[4/4] Creating custom investment recommendation dataset...\")\n\n\ncustom_investment_data = [\n    # ==================== COMPANY ANALYSIS (15 examples) ====================\n    {\n        \"instruction\": \"Should I invest in this company?\",\n        \"input\": \"Tech company, Revenue growth: 25% YoY, P/E ratio: 45, Debt-to-Equity: 0.3, Market cap: $50B\",\n        \"output\": \"POSITIVE OUTLOOK - Consider investing. The 25% revenue growth indicates strong market demand. P/E of 45 is elevated but reasonable for high-growth tech. Low debt-to-equity (0.3) shows financial stability. Risk: High P/E means vulnerability to market corrections. Recommendation: Suitable for growth-oriented portfolios with 3-5 year horizon. Disclaimer: Not financial advice.\"\n    },\n    {\n        \"instruction\": \"Analyze this company's financial health\",\n        \"input\": \"Manufacturing company, Operating margin: 8%, Current ratio: 1.5, ROE: 12%, Industry avg ROE: 10%\",\n        \"output\": \"MODERATE HEALTH - The company shows decent performance. Operating margin of 8% is acceptable for manufacturing. Current ratio of 1.5 indicates adequate liquidity. ROE of 12% exceeds industry average (10%), suggesting efficient capital use. Consider for value portfolios. Monitor industry trends and competition.\"\n    },\n    {\n        \"instruction\": \"Should I buy this stock?\",\n        \"input\": \"Pharmaceutical company, P/E: 18, Revenue decline: -5% YoY, Strong pipeline, Cash reserves: $15B, Debt: $8B\",\n        \"output\": \"CAUTIOUSLY OPTIMISTIC - The revenue decline is concerning but may be temporary. P/E of 18 is reasonable for pharma. Strong cash position ($15B vs $8B debt) provides safety net. Pipeline strength is critical for future growth. Recommendation: Wait for pipeline milestones before investing. Suitable for risk-tolerant investors with 5+ year horizon.\"\n    },\n    {\n        \"instruction\": \"Evaluate this company\",\n        \"input\": \"Retail company, Gross margin: 35%, Operating margin: 4%, Inventory turnover: 6x, Same-store sales: +3%\",\n        \"output\": \"MIXED SIGNALS - Gross margin of 35% is healthy, but operating margin of 4% is thin for retail. Inventory turnover of 6x is decent. Same-store sales growth of 3% shows modest momentum. Concerns: Low operating leverage, vulnerable to cost pressures. Recommendation: Suitable only for value investors seeking turnaround plays.\"\n    },\n    {\n        \"instruction\": \"Analyze this energy company\",\n        \"input\": \"Oil & Gas company, P/B ratio: 0.8, Dividend yield: 5%, Debt-to-EBITDA: 2.5, Oil price exposure: High\",\n        \"output\": \"VALUE OPPORTUNITY WITH RISKS - P/B below 1.0 suggests undervaluation. 5% dividend yield is attractive for income investors. Debt-to-EBITDA of 2.5 is manageable. Major risk: High oil price volatility. Recommendation: Suitable for value/income portfolios with commodity risk tolerance. Consider 5-10% position size.\"\n    },\n    {\n        \"instruction\": \"Should I invest in this bank?\",\n        \"input\": \"Regional bank, Tier 1 capital ratio: 12%, NPL ratio: 2%, Net interest margin: 3.5%, Deposit growth: 8%\",\n        \"output\": \"SOLID FUNDAMENTALS - Tier 1 capital of 12% exceeds regulatory minimums (shows strength). NPL ratio of 2% is acceptable. Net interest margin of 3.5% is healthy. Strong deposit growth of 8% indicates customer confidence. Recommendation: Good defensive play for income portfolios. Monitor interest rate environment.\"\n    },\n    {\n        \"instruction\": \"Evaluate this software company\",\n        \"input\": \"SaaS company, Revenue growth: 40%, Gross margin: 80%, Operating margin: -15%, Customer retention: 95%\",\n        \"output\": \"HIGH GROWTH, PRE-PROFIT - 40% revenue growth and 80% gross margin are excellent for SaaS. Negative operating margin is common in growth phase. 95% retention indicates strong product-market fit. Risk: Path to profitability unclear. Recommendation: Suitable for aggressive growth portfolios with 5+ year horizon. Monitor burn rate.\"\n    },\n    {\n        \"instruction\": \"Analyze this automotive stock\",\n        \"input\": \"EV manufacturer, Production capacity: 500k units/year, Backlog: 400k orders, Cash burn: $2B/quarter\",\n        \"output\": \"EXECUTION RISK HIGH - Strong demand (400k backlog) validates product. Production capacity adequate short-term. Critical concern: $2B quarterly cash burn requires frequent capital raises. Risk: Dilution, bankruptcy if funding dries up. Recommendation: Only for high-risk portfolios. Wait for positive free cash flow.\"\n    },\n    {\n        \"instruction\": \"Should I buy this real estate stock?\",\n        \"input\": \"REIT, FFO yield: 6%, Occupancy: 92%, Debt-to-asset ratio: 45%, Property type: Office\",\n        \"output\": \"ATTRACTIVE YIELD WITH CONCERNS - 6% FFO yield is compelling for income. 92% occupancy is decent. Debt at 45% is moderate. Major concern: Office sector faces structural headwinds from remote work. Recommendation: Consider only as small position (<5%) in diversified REIT portfolio. Prefer residential/industrial REITs.\"\n    },\n    {\n        \"instruction\": \"Evaluate this consumer goods company\",\n        \"input\": \"CPG company, Brand strength: High, Revenue growth: 3%, Operating margin: 18%, Market share: Declining\",\n        \"output\": \"MATURE WITH HEADWINDS - Strong brands provide moat. Low growth (3%) typical for mature CPG. Healthy operating margin of 18%. Declining market share is concerning - suggests competitive pressures. Recommendation: Hold if already owned, but better opportunities exist in growth sectors. Good for defensive portfolios.\"\n    },\n    {\n        \"instruction\": \"Analyze semiconductor company\",\n        \"input\": \"Chip manufacturer, R&D spend: 20% of revenue, P/E: 25, Cyclical industry, Leading technology node\",\n        \"output\": \"QUALITY CYCLICAL - High R&D (20%) ensures technology leadership. P/E of 25 is reasonable given growth. Cyclical nature means timing matters. Leading node position provides competitive advantage. Recommendation: Buy during industry downturns when P/E compresses. Suitable for tech-focused portfolios with 3-5 year view.\"\n    },\n    {\n        \"instruction\": \"Should I invest in this telecom company?\",\n        \"input\": \"Telecom provider, Dividend yield: 7%, Revenue flat, High capex needs, 5G rollout ongoing\",\n        \"output\": \"INCOME PLAY WITH RISKS - 7% dividend yield attracts income investors. Flat revenue indicates mature market. High capex for 5G pressures cash flow - watch dividend sustainability. Recommendation: Suitable for income portfolios but limit position size to 5%. Monitor free cash flow coverage of dividend.\"\n    },\n    {\n        \"instruction\": \"Evaluate this biotech stock\",\n        \"input\": \"Biotech company, Phase 3 trial results: Positive, Market size: $10B, Competition: Moderate, Cash runway: 2 years\",\n        \"output\": \"HIGH RISK, HIGH REWARD - Positive Phase 3 is major catalyst. Large market ($10B) offers significant upside. Moderate competition is favorable. 2-year cash runway sufficient for commercialization. Risks: FDA approval uncertainty, reimbursement challenges. Recommendation: Speculative position (2-3%) for growth portfolios. Exit if FDA approval fails.\"\n    },\n    {\n        \"instruction\": \"Analyze this airline stock\",\n        \"input\": \"Airline, Debt-to-equity: 5.0, Fuel hedges: 60% for next year, Load factor: 85%, Cost per ASM: Competitive\",\n        \"output\": \"HIGH LEVERAGE, OPERATIONALLY SOUND - Debt-to-equity of 5.0 is concerning (highly leveraged). Good fuel hedging (60%) reduces near-term risk. Strong load factor (85%) shows demand. Competitive costs are positive. Major risk: Economic downturn would be devastating. Recommendation: Avoid unless very bullish on economy. Too risky for most portfolios.\"\n    },\n    {\n        \"instruction\": \"Should I buy this utility stock?\",\n        \"input\": \"Electric utility, Regulated business: 100%, ROE: 9%, Dividend yield: 4%, Rate case pending\",\n        \"output\": \"STABLE DEFENSIVE INVESTMENT - Fully regulated provides predictable cash flows. ROE of 9% is typical for utilities. 4% dividend yield offers income. Pending rate case is key catalyst - approval would support dividend growth. Recommendation: Excellent for conservative/income portfolios. Low risk, low return profile. 5-15% allocation reasonable.\"\n    },\n    \n    # ==================== FINANCIAL RATIOS & METRICS (15 examples) ====================\n    {\n        \"instruction\": \"What are red flags in financial statements?\",\n        \"input\": \"\",\n        \"output\": \"Key red flags to watch: 1) Declining revenue with increasing receivables (fake sales), 2) Operating cash flow < Net income (earnings manipulation), 3) High debt-to-equity ratio >2.0 (financial risk), 4) Frequent auditor changes (governance issues), 5) Unusual related-party transactions, 6) Inventory growing faster than sales (obsolete stock). Always cross-verify with multiple quarters of data.\"\n    },\n    {\n        \"instruction\": \"Explain Price-to-Earnings ratio\",\n        \"input\": \"\",\n        \"output\": \"P/E ratio = Stock Price ÷ Earnings Per Share. It shows how much investors pay for each dollar of earnings. Example: P/E of 20 means investors pay $20 for $1 of annual earnings. Low P/E (<15): Potentially undervalued or mature company. High P/E (>25): Growth expectations or overvaluation. Compare within same industry. Tech averages 30+, utilities average 15-20.\"\n    },\n    {\n        \"instruction\": \"What is a good current ratio?\",\n        \"input\": \"\",\n        \"output\": \"Current ratio = Current Assets ÷ Current Liabilities. Measures short-term liquidity. Ideal range: 1.5-3.0. Below 1.0: Liquidity risk - company may struggle to pay short-term obligations. 1.0-1.5: Acceptable but monitor closely. Above 3.0: Excess cash - inefficient capital use. Industry matters: Retailers can operate at lower ratios than manufacturers.\"\n    },\n    {\n        \"instruction\": \"Explain debt-to-equity ratio\",\n        \"input\": \"\",\n        \"output\": \"Debt-to-equity = Total Debt ÷ Shareholders' Equity. Measures financial leverage. Low (<0.5): Conservative, low risk. Moderate (0.5-1.5): Balanced capital structure. High (>2.0): Risky - vulnerable to downturns. Industry context critical: Utilities/REITs naturally higher, tech typically lower. Compare to industry peers, not absolute numbers.\"\n    },\n    {\n        \"instruction\": \"What is Return on Equity (ROE)?\",\n        \"input\": \"\",\n        \"output\": \"ROE = Net Income ÷ Shareholders' Equity. Measures profitability relative to equity. Good ROE: >15% consistently. Average: 10-15%. Poor: <10%. High ROE can indicate: 1) Strong profitability, or 2) High leverage (risky). Use DuPont analysis to decompose: ROE = Profit Margin × Asset Turnover × Equity Multiplier. Compare to industry average.\"\n    },\n    {\n        \"instruction\": \"Explain Free Cash Flow\",\n        \"input\": \"\",\n        \"output\": \"Free Cash Flow (FCF) = Operating Cash Flow - Capital Expenditures. Shows cash available for dividends, buybacks, debt repayment. Positive FCF: Healthy - company generates more cash than needed for operations. Negative FCF: May indicate growth investments (good) or operational problems (bad). FCF > Net Income is ideal - shows quality earnings.\"\n    },\n    {\n        \"instruction\": \"What is Price-to-Book ratio?\",\n        \"input\": \"\",\n        \"output\": \"P/B ratio = Stock Price ÷ Book Value per Share. Compares market value to accounting value. P/B < 1.0: Potentially undervalued (value opportunity) or distressed. P/B 1-3: Typical for most companies. P/B > 5: Premium valuation - common for high-ROE businesses. Best for capital-intensive industries (banks, manufacturing). Less useful for asset-light tech companies.\"\n    },\n    {\n        \"instruction\": \"Explain PEG ratio\",\n        \"input\": \"\",\n        \"output\": \"PEG = P/E Ratio ÷ Earnings Growth Rate. Adjusts P/E for growth. PEG < 1.0: Potentially undervalued relative to growth. PEG = 1.0: Fairly valued. PEG > 2.0: Expensive relative to growth prospects. Example: P/E of 30 with 30% growth = PEG of 1.0 (fair). Limitations: Assumes linear growth, ignores quality/risk differences.\"\n    },\n    {\n        \"instruction\": \"What is dividend yield?\",\n        \"input\": \"\",\n        \"output\": \"Dividend Yield = Annual Dividend per Share ÷ Stock Price. Shows income return. Low yield (<2%): Growth companies reinvesting profits. Moderate (2-4%): Balanced approach. High (>5%): Income focus, or potential dividend cut risk. Warning: Unusually high yields (>8%) often signal dividend sustainability concerns. Consider payout ratio alongside yield.\"\n    },\n    {\n        \"instruction\": \"Explain operating margin\",\n        \"input\": \"\",\n        \"output\": \"Operating Margin = Operating Income ÷ Revenue. Measures operational efficiency. Excellent: >20%. Good: 10-20%. Average: 5-10%. Poor: <5%. High margins indicate: pricing power, economies of scale, efficient operations. Expanding margins are bullish. Contracting margins signal competitive pressures or rising costs. Industry comparison essential.\"\n    },\n    {\n        \"instruction\": \"What is Enterprise Value?\",\n        \"input\": \"\",\n        \"output\": \"EV = Market Cap + Total Debt - Cash. Represents takeover cost. More accurate than market cap for valuation. EV/EBITDA is preferred multiple (compare companies with different capital structures). Example: Company A: $100M market cap, $50M debt, $10M cash = $140M EV. Lower EV/EBITDA suggests better value. Typical range: 8-12x for mature companies.\"\n    },\n    {\n        \"instruction\": \"Explain gross margin\",\n        \"input\": \"\",\n        \"output\": \"Gross Margin = (Revenue - COGS) ÷ Revenue. Measures production efficiency. Software/SaaS: 70-90% (excellent). Retail: 20-40%. Manufacturing: 30-50%. Grocery: 10-20%. Increasing gross margin indicates: pricing power, scale benefits, product mix improvement. Declining margin signals: competition, input cost inflation, product commoditization.\"\n    },\n    {\n        \"instruction\": \"What is asset turnover ratio?\",\n        \"input\": \"\",\n        \"output\": \"Asset Turnover = Revenue ÷ Total Assets. Measures asset efficiency. High ratio (>2.0): Efficient asset use - common in retail, service companies. Low ratio (<0.5): Capital intensive - utilities, real estate, manufacturing. Improving trend is positive. Declining trend suggests: excess capacity, outdated assets, or growth slowdown. Industry context critical.\"\n    },\n    {\n        \"instruction\": \"Explain interest coverage ratio\",\n        \"input\": \"\",\n        \"output\": \"Interest Coverage = EBIT ÷ Interest Expense. Measures ability to pay interest. Strong: >5x (very safe). Adequate: 2.5-5x. Weak: 1.5-2.5x (vulnerable).危险: <1.5x (distress risk). Below 1.0 means company can't cover interest from operations. Monitor trend: declining coverage signals increasing financial stress. Critical for highly leveraged companies.\"\n    },\n    {\n        \"instruction\": \"What is quick ratio?\",\n        \"input\": \"\",\n        \"output\": \"Quick Ratio = (Current Assets - Inventory) ÷ Current Liabilities. More conservative than current ratio (excludes inventory). Good: >1.0. Adequate: 0.7-1.0. Concerning: <0.7. Useful for companies with slow-moving inventory. Difference between current ratio and quick ratio highlights inventory dependency. If quick ratio << current ratio, investigate inventory quality.\"\n    },\n    \n    # ==================== PORTFOLIO MANAGEMENT (15 examples) ====================\n    {\n        \"instruction\": \"How to diversify investment portfolio?\",\n        \"input\": \"Age: 30, Risk tolerance: Moderate, Investment horizon: 20 years\",\n        \"output\": \"Recommended allocation for 30-year-old with moderate risk: 60% stocks (40% domestic large-cap, 15% international, 5% small-cap), 30% bonds (20% government, 10% corporate), 10% alternatives (REITs, commodities). Rebalance annually. Given 20-year horizon, can tolerate more equity risk. Gradually shift to conservative allocation approaching retirement. Dollar-cost average monthly to reduce timing risk.\"\n    },\n    {\n        \"instruction\": \"What is asset allocation?\",\n        \"input\": \"\",\n        \"output\": \"Asset allocation divides portfolio among asset classes (stocks, bonds, real estate, cash). Principle: Different assets perform differently under various conditions. Typical allocations: Aggressive (90% stocks, 10% bonds), Moderate (60% stocks, 40% bonds), Conservative (30% stocks, 70% bonds). Asset allocation explains 90% of portfolio returns. Rebalance annually to maintain target weights.\"\n    },\n    {\n        \"instruction\": \"How many stocks should I own?\",\n        \"input\": \"\",\n        \"output\": \"Optimal diversification: 15-30 stocks across different sectors. Research shows: 15 stocks eliminate ~90% of company-specific risk. 30 stocks: ~95% diversification benefit. More stocks: Diminishing returns, harder to monitor. Too few (<10): Excess concentration risk. Too many (>50): Closet indexing - just buy index fund. Focus on quality over quantity.\"\n    },\n    {\n        \"instruction\": \"What is rebalancing?\",\n        \"input\": \"\",\n        \"output\": \"Rebalancing restores target asset allocation by selling outperformers and buying underperformers. Example: Target 60% stocks/40% bonds. After year: 70%/30% (stocks rallied). Rebalance: Sell 10% stocks, buy bonds. Benefits: Maintains risk level, forces 'buy low, sell high' discipline. Frequency: Annually or when drift exceeds 5%. Avoid over-rebalancing (tax costs, fees).\"\n    },\n    {\n        \"instruction\": \"Should I use dollar-cost averaging?\",\n        \"input\": \"\",\n        \"output\": \"Dollar-cost averaging (DCA): Invest fixed amount regularly regardless of price. Benefits: Reduces timing risk, removes emotion, builds discipline. Drawback: Mathematically inferior to lump-sum in rising markets. Best for: Regular salary investors, those afraid of volatility, beginners. Lump-sum better if: You have cash now, long time horizon, can tolerate volatility. Psychological benefit often outweighs mathematical cost.\"\n    },\n    {\n        \"instruction\": \"What is sector rotation?\",\n        \"input\": \"\",\n        \"output\": \"Sector rotation: Shifting investments among sectors based on economic cycle. Early recovery: Financials, industrials. Mid cycle: Technology, consumer discretionary. Late cycle: Energy, materials. Recession: Utilities, consumer staples, healthcare. Challenge: Requires accurate economic forecasting (difficult). Alternative: Equal-weight sector exposure for simplicity. Most individual investors should avoid active rotation.\"\n    },\n    {\n        \"instruction\": \"How to invest during market correction?\",\n        \"input\": \"Market down 15% from peak\",\n        \"output\": \"Correction strategy: 1) Don't panic sell - corrections are normal (average once per year). 2) Review portfolio - ensure diversification adequate. 3) Rebalance - buy stocks if below target allocation. 4) Deploy cash reserves - scale into quality names. 5) Avoid catching falling knives - wait for stabilization. 6) Check fundamentals - ensure holdings still solid. Historical fact: Markets always recover long-term.\"\n    },\n    {\n        \"instruction\": \"What percentage should be in international stocks?\",\n        \"input\": \"\",\n        \"output\": \"International allocation: 20-40% of equity portfolio. Benefits: Geographic diversification, access to growth markets, currency diversification. Risks: Political instability, currency fluctuations, less regulation. Allocation approach: 25% international as baseline. Increase to 40% if: Seeking growth, young investor. Decrease to 10-20% if: Near retirement, prefer stability. Consider both developed (Europe, Japan) and emerging markets.\"\n    },\n    {\n        \"instruction\": \"Should I invest in individual stocks or index funds?\",\n        \"input\": \"\",\n        \"output\": \"Index funds better for most investors: Lower fees (0.05% vs 1%+), automatic diversification, match market returns (beats 85% of active managers long-term), tax efficient. Individual stocks better if: You have time/expertise to research, enjoy investing, can handle volatility. Hybrid approach: 70-80% index funds (core), 20-30% individual stocks (satellite). Never invest in individual stocks unless you understand the business.\"\n    },\n    {\n        \"instruction\": \"How to build emergency fund?\",\n        \"input\": \"\",\n        \"output\": \"Emergency fund guidelines: Amount: 3-6 months of expenses (6-12 months if self-employed). Location: High-yield savings account (liquid, FDIC insured). Don't invest emergency fund in stocks. Build gradually: Start with $1,000, then add monthly until target reached. Priority: Build emergency fund BEFORE investing in stocks. Replenish immediately after use. This prevents forced stock sales during emergencies.\"\n    },\n    {\n        \"instruction\": \"What is the 60/40 portfolio?\",\n        \"input\": \"\",\n        \"output\": \"60/40 portfolio: 60% stocks, 40% bonds. Traditional balanced allocation providing: Growth potential (stocks), stability (bonds), income (bond interest), lower volatility than 100% stocks. Historical returns: ~8% annually with ~10% max drawdown. Criticisms: Low bond yields today reduce effectiveness. Alternatives: 70/30 for younger investors, 50/50 for retirees. Still valid as core strategy despite challenges.\"\n    },\n    {\n        \"instruction\": \"How to invest in retirement accounts?\",\n        \"input\": \"Age: 35, 401(k) available, IRA available\",\n        \"output\": \"Retirement account priority: 1) 401(k) up to employer match (free money), 2) Max Roth IRA ($6,500/year in 2023), 3) Max 401(k) ($22,500/year), 4) Taxable brokerage. Asset location: Bonds in 401(k)/IRA (tax-inefficient), stocks in taxable (tax-efficient). Age 35: 85% stocks, 15% bonds. Target date 2055 fund is reasonable alternative if prefer simplicity. Increase bond allocation 1% per year after age 40.\"\n    },\n    {\n        \"instruction\": \"What is tax-loss harvesting?\",\n        \"input\": \"\",\n        \"output\": \"Tax-loss harvesting: Sell losing investments to offset capital gains taxes. Process: Sell position at loss, immediately buy similar (not identical) security to maintain exposure. Benefit: Reduce taxes, improve after-tax returns. Example: Sell stock A at $3k loss, buy similar stock B. Use $3k loss to offset gains elsewhere. Rules: Avoid wash sale (30-day rule), only in taxable accounts. Most effective in volatile markets.\"\n    },\n    {\n        \"instruction\": \"Should I invest in growth or value stocks?\",\n        \"input\": \"\",\n        \"output\": \"Growth vs Value: Growth stocks: High valuations, rapid earnings growth, tech-heavy (NASDAQ). Value stocks: Low valuations (P/E, P/B), mature companies, dividend-paying. Performance: Alternates - value outperforms in rising rate environments, growth in falling rates. Recommendation: Own both (50/50) for diversification. Tilt toward growth if young (<40), value if near retirement. Total market index fund holds both automatically.\"\n    },\n    {\n        \"instruction\": \"What is portfolio withdrawal rate in retirement?\",\n        \"input\": \"\",\n        \"output\": \"4% rule: Withdraw 4% of portfolio in year 1, adjust for inflation annually. Research shows: 4% rate has 95% success over 30 years. Conservative: 3-3.5% (higher success rate). Aggressive: 5%+ (higher failure risk). Factors affecting rate: Portfolio allocation (more stocks = higher sustainable rate), retirement length, spending flexibility, Social Security income. Monitor annually, adjust if portfolio declines significantly.\"\n    },\n    \n    # ==================== INVESTMENT STRATEGIES (15 examples) ====================\n    {\n        \"instruction\": \"What is value investing?\",\n        \"input\": \"\",\n        \"output\": \"Value investing: Buy undervalued companies trading below intrinsic value. Key metrics: Low P/E, P/B, P/S ratios. Look for: Strong balance sheets, consistent earnings, competitive advantages. Famous practitioners: Warren Buffett, Benjamin Graham. Process: Calculate intrinsic value, buy at discount (margin of safety), hold until market recognizes value. Time horizon: 3-5+ years. Risks: Value traps (cheap for a reason), requires patience, can underperform growth for years.\"\n    },\n    {\n        \"instruction\": \"Explain momentum investing\",\n        \"input\": \"\",\n        \"output\": \"Momentum investing: Buy assets showing upward price trends, assuming trends persist. Strategy: Identify stocks with strong 6-12 month returns, ride the trend. Works because: Herding behavior, under-reaction to news, institutional buying. Risks: Sudden reversals, buying high and selling higher requires discipline. Combine with: Stop-losses, position sizing. Academic evidence: Momentum premium exists but volatile. Suitable for: Active traders, not buy-and-hold investors.\"\n    },\n    {\n        \"instruction\": \"What is dividend investing?\",\n        \"input\": \"\",\n        \"output\": \"Dividend investing: Focus on stocks paying regular dividends. Benefits: Steady income, lower volatility, forced discipline (companies must generate cash). Target: Dividend yield 2-4%, payout ratio <60%, 10+ year growth history (Dividend Aristocrats). Sectors: Utilities, consumer staples, REITs, telecom. Reinvest dividends when young, take as income in retirement. Risk: High yields may signal dividend cut risk. Tax: Qualified dividends taxed favorably.\"\n    },\n    {\n        \"instruction\": \"Should I invest in index funds?\",\n        \"input\": \"\",\n        \"output\": \"Index funds strongly recommended for most investors. Benefits: Ultra-low fees (0.03-0.1%), instant diversification, match market returns, tax efficient, no research needed. Evidence: Over 15+ years, index funds beat 85-90% of actively managed funds. Allocation: Total stock market index (70%), international index (20%), bond index (10%). Pioneers: Vanguard, Fidelity, Schwab. Suitable for: Everyone, especially beginners. Weakness: Can't outperform market (but most who try fail).\"\n    },\n    {\n        \"instruction\": \"What is contrarian investing?\",\n        \"input\": \"\",\n        \"output\": \"Contrarian investing: Go against prevailing market sentiment. Philosophy: 'Be fearful when others are greedy, greedy when others are fearful.' Strategy: Buy during panic (2008, 2020), sell during euphoria (1999, 2021). Indicators: VIX (fear gauge), sentiment surveys, put/call ratios. Difficulty: Emotionally challenging, requires conviction, early entry possible (catching falling knife). Success: Warren Buffett (bought during 2008), Howard Marks. Not suitable for: Beginners, those needing near-term returns.\"\n    },\n    {\n        \"instruction\": \"Explain dollar-cost averaging vs lump sum\",\n        \"input\": \"\",\n        \"output\": \"Dollar-cost averaging (DCA): Invest regularly over time. Lump sum: Invest all at once. Mathematical fact: Lump sum wins ~66% of time (markets trend up). DCA advantages: Reduces timing risk, psychologically easier, smooths entry price. DCA disadvantages: Miss potential gains, more fees/taxes. Recommendation: Lump sum if you can handle volatility and have long horizon. DCA if: Investing large windfall, risk-averse, market at all-time highs. Compromise: Invest 50% immediately, DCA remainder over 6 months.\"\n    },\n    {\n        \"instruction\": \"What is buy and hold strategy?\",\n        \"input\": \"\",\n        \"output\": \"Buy and hold: Purchase quality investments and hold long-term (10+ years) regardless of volatility. Benefits: Minimize taxes, avoid timing mistakes, reduce fees, capture compound growth. Requirements: Emotional discipline, quality stock selection, periodic review (not blind holding). Evidence: Outperforms active trading for most investors. Example: $10k in S&P 500 in 2000 = $40k+ in 2023 despite crashes. Selling triggers: Broken thesis, better opportunity, need cash. Not suitable for: Active traders, those needing short-term returns.\"\n    },\n    {\n        \"instruction\": \"Should I try to time the market?\",\n        \"input\": \"\",\n        \"output\": \"Market timing NOT recommended. Evidence: Missing best 10 days over 30 years reduces returns by 50%. Challenges: Must be right twice (when to exit AND re-enter), emotionally difficult (fear/greed), transaction costs. Statistics: Even professionals fail consistently. Alternative: Stay invested, rebalance regularly, have cash reserve for opportunities. Exception: Tactical shifts okay (reducing stocks when extremely overvalued) but avoid market timing as core strategy. 'Time IN the market beats timing the market.'\"\n    },\n    {\n        \"instruction\": \"What is dollar-cost averaging?\",\n        \"input\": \"\",\n        \"output\": \"Dollar-cost averaging (DCA): Invest fixed amount at regular intervals regardless of price. Mechanics: Buy more shares when prices low, fewer when high - averages purchase price. Example: Invest $500/month for 12 months instead of $6,000 upfront. Benefits: Reduces timing risk, removes emotion, builds discipline. Best for: Volatile markets, beginners, regular savers. Drawback: If market rises steadily, DCA underperforms lump sum. Verdict: Good for most individual investors with regular income.\"\n    },\n    {\n        \"instruction\": \"Explain growth investing\",\n        \"input\": \"\",\n        \"output\": \"Growth investing: Target companies with above-average growth potential. Characteristics: High revenue/earnings growth (20%+), expanding margins, innovative products, large addressable markets. Typical valuations: High P/E (30-50+), Price-to-Sales focus. Sectors: Technology, biotech, consumer internet. Risks: Expensive valuations (vulnerable to corrections), high volatility, competition. Examples: Amazon (early 2000s), Tesla. Suitable for: Long-term investors (5+ years), high risk tolerance, belief in disruptive innovation. Compare to value: Higher returns but higher risk.\"\n    },\n    {\n        \"instruction\": \"What is GARP investing?\",\n        \"input\": \"\",\n        \"output\": \"GARP (Growth At Reasonable Price): Blend of growth and value investing. Criteria: PEG ratio < 1.5, earnings growth 15-25%, reasonable valuations (P/E 15-25). Philosophy: Don't overpay for growth, but accept premium for quality. Popularized by: Peter Lynch. Advantages: Balances growth potential with valuation discipline, lower risk than pure growth. Process: Screen for earnings growth, filter by PEG ratio, analyze fundamentals. Suitable for: Moderate risk investors seeking growth with downside protection.\"\n    },\n    {\n        \"instruction\": \"Should I invest in ESG funds?\",\n        \"input\": \"\",\n        \"output\": \"ESG (Environmental, Social, Governance) investing: Integrate sustainability factors into investment decisions. Types: Exclusionary (avoid tobacco, weapons), Best-in-class (leaders in ESG), Impact (measurable social benefit). Performance: Mixed evidence - some studies show outperformance, others neutral. Benefits: Align investments with values, potential risk mitigation (avoid regulatory issues). Drawbacks: Higher fees, may miss certain sectors (energy), 'greenwashing' concerns. Recommendation: If personally important, allocate 10-25% to ESG funds alongside core holdings.\"\n    },\n    {\n        \"instruction\": \"What is tactical asset allocation?\",\n        \"input\": \"\",\n        \"output\": \"Tactical asset allocation: Temporarily deviate from strategic allocation to exploit market opportunities. Strategy: Overweight undervalued assets, underweight expensive ones. Example: Strategic allocation 60% stocks/40% bonds. Market crashes → shift to 70% stocks temporarily. Differences from strategic: Active adjustments (quarterly/annually) vs fixed weights. Requirements: Market analysis skills, discipline to revert. Risks: Timing errors, overtrading. Recommendation: Small tactical tilts (±5-10%) acceptable, large shifts risky for individuals.\"\n    },\n    {\n        \"instruction\": \"Explain passive vs active investing\",\n        \"input\": \"\",\n        \"output\": \"Passive investing: Buy index funds, match market returns, minimal trading. Active investing: Pick stocks/funds trying to beat market. Data: Over 15 years, ~85% of active managers underperform passive. Passive advantages: Lower fees (0.05% vs 1%+), tax efficient, less time. Active advantages: Potential outperformance, downside protection, human judgment. Recommendation: Most investors should be 80-100% passive. Reserve 10-20% for active if you enjoy stock picking, have expertise. Hybrid portfolios common: Core passive + satellite active.\"\n    },\n    {\n        \"instruction\": \"What is factor investing?\",\n        \"input\": \"\",\n        \"output\": \"Factor investing: Target specific drivers of returns beyond market exposure. Major factors: Value (cheap stocks), Momentum (trending stocks), Quality (profitable, stable), Size (small-cap), Low volatility. Implementation: Factor-tilted index funds or ETFs. Evidence: Academic research supports factor premiums over long periods. Multi-factor approach: Combine 3-4 factors for diversification. Risks: Factors can underperform for years, higher fees than plain index. Suitable for: Sophisticated investors seeking enhanced returns with controlled risk. Alternative to active management.\"\n    },\n    \n    # ==================== RETIREMENT PLANNING (10 examples) ====================\n    {\n        \"instruction\": \"How much to save for retirement?\",\n        \"input\": \"Age: 30, Current income: $80k, Desired retirement age: 65\",\n        \"output\": \"Retirement savings target: 25x annual expenses (4% withdrawal rule). If need $60k/year in retirement → save $1.5M. Rule of thumb by age 30: Save 1x salary ($80k). Savings rate: Aim for 15-20% of gross income. With 35 years to retirement at $80k income: Save $1,000/month ($12k/year) at 7% return → $1.8M by 65. Increase savings 1% annually. Max 401(k) contributions ($22,500/year). Start with employer match minimum, increase over time.\"\n    },\n    {\n        \"instruction\": \"What is the 4% rule?\",\n        \"input\": \"\",\n        \"output\": \"4% rule: Withdraw 4% of portfolio in first retirement year, adjust for inflation annually. Example: $1M portfolio → withdraw $40k year 1, $41.2k year 2 (3% inflation). Research: 95% success rate over 30-year retirement (Trinity Study). Portfolio: 50-75% stocks, 25-50% bonds. Conservative: Use 3-3.5% for longer retirement (40+ years). Flexible: Reduce spending in down markets. Limitations: Assumes static spending, ignores Social Security, doesn't consider sequence risk.\"\n    },\n    {\n        \"instruction\": \"When should I take Social Security?\",\n        \"input\": \"Full retirement age: 67\",\n        \"output\": \"Social Security timing: Age 62 (earliest): Reduced by ~30%. Age 67 (FRA): 100% benefit. Age 70 (latest): Increased by ~24%. Decision factors: Break-even at ~80 years old (early vs delayed). Take early if: Poor health, need money now, don't expect to live past 75. Take late if: Healthy, have other income, expect longevity. Married: Higher earner should usually delay (spousal benefits). Run personal calculation at ssa.gov. Delaying = free 8%/year return.\"\n    },\n    {\n        \"instruction\": \"Roth IRA vs Traditional IRA?\",\n        \"input\": \"Income: $75k, Tax bracket: 22%\",\n        \"output\": \"Traditional IRA: Tax deduction now, taxed in retirement. Roth IRA: No deduction, tax-free withdrawals. Choose Traditional if: High tax bracket now (>24%), expect lower bracket in retirement, need deduction to afford contribution. Choose Roth if: Low/moderate bracket now (<22%), expect higher bracket in retirement, young (decades of tax-free growth). Income limits: Roth phaseout at $138k-$153k (single). Recommendation at $75k/22% bracket: Roth IRA (likely in higher bracket later). Hedge: Split 50/50.\"\n    },\n    {\n        \"instruction\": \"How to catch up on retirement savings at 50?\",\n        \"input\": \"Age: 50, Retirement savings: $200k, Target: $1M by 65\",\n        \"output\": \"Catch-up strategy (need $800k more in 15 years): Max 401(k) with catch-up ($30k/year age 50+), Max IRA with catch-up ($7,500/year), Total: $37,500/year. At 7% return: $37k/year × 15 years = $950k (close to goal). Additional steps: Delay retirement to 67-70, Downsize home, Reduce expenses now, Consider part-time work in retirement, Boost savings rate to 30-40%, Avoid withdrawing from retirement accounts. Social Security will supplement. Still achievable but requires discipline.\"\n    },\n    {\n        \"instruction\": \"What is sequence of returns risk?\",\n        \"input\": \"\",\n        \"output\": \"Sequence risk: Order of investment returns matters in retirement. Problem: Market crash early in retirement is devastating (selling low locks in losses). Example: Two retirees, same average return, different sequences → vastly different outcomes. Mitigation: Hold 2-3 years expenses in cash/bonds (don't sell stocks in downturn), Use bucket strategy (cash/bonds/stocks), Reduce equity exposure in retirement (60/40 → 50/50), Flexible spending (cut in down years). Most dangerous: 5 years before and 10 years after retirement. Bond tent strategy recommended.\"\n    },\n    {\n        \"instruction\": \"Should I pay off mortgage before retiring?\",\n        \"input\": \"Age: 60, Mortgage balance: $150k, Rate: 3%, Retirement: 65\",\n        \"output\": \"Pay off mortgage: Pros: Guaranteed return (3%), psychological peace, lower retirement expenses. Cons: Lose tax deduction, illiquid (money stuck in home), opportunity cost if stocks return more. Decision framework: Pay off if: Mortgage rate > 4%, risk-averse, have sufficient retirement savings. Keep if: Rate < 3%, need liquidity, comfortable with leverage. Compromise: Pay extra principal (half to mortgage, half to investments). At 3% rate: Keep mortgage, invest instead (expect 7%+ from stocks long-term). Prioritize emotional comfort.\"\n    },\n    {\n        \"instruction\": \"What is a retirement bucket strategy?\",\n        \"input\": \"\",\n        \"output\": \"Bucket strategy divides portfolio by time horizon: Bucket 1 (Years 1-3): Cash/money market → $90k (3 years expenses), immediate needs, no market risk. Bucket 2 (Years 4-10): Bonds/conservative → $210k (7 years expenses), refill bucket 1 annually. Bucket 3 (Years 11+): Stocks → $700k+, long-term growth, don't touch for 10+ years. Benefits: Psychological comfort (cash cushion), avoid selling stocks in downturn, clear structure. Rebalancing: Refill bucket 1 from bucket 2 annually, replenish bucket 2 from bucket 3 when stocks perform well. Popular for retirees.\"\n    },\n    {\n        \"instruction\": \"How much does healthcare cost in retirement?\",\n        \"input\": \"\",\n        \"output\": \"Retirement healthcare costs: Estimated $315k per couple (age 65+) excluding long-term care. Breakdown: Medicare premiums ($200-300/month), Medigap/supplemental ($100-300/month), Out-of-pocket expenses, Prescription drugs. Long-term care: Average $100k/year for nursing home. Strategies: Contribute to HSA while working (triple tax advantaged), Budget 15% of retirement expenses for healthcare, Consider long-term care insurance (buy at age 55-60), Medicare starts age 65 (bridge gap if retire early), Medicaid for low-income. Healthcare inflation runs 5-6% (higher than general inflation).\"\n    },\n    {\n        \"instruction\": \"Should I convert Traditional IRA to Roth?\",\n        \"input\": \"Age: 55, Traditional IRA: $500k, Current tax bracket: 24%\",\n        \"output\": \"Roth conversion considerations: Pay taxes now at 24%, tax-free withdrawals later. Convert if: Expect higher tax bracket in retirement, have cash to pay conversion taxes (don't withdraw from IRA), years until retirement (let Roth grow), want tax diversification. Don't convert if: High bracket now, low income years coming (unemployment, retirement), can't afford taxes, RMDs won't push to higher bracket. Strategy: Partial conversions over multiple years (fill up 24% bracket), Convert during low-income years (between retirement and Social Security), Run tax projections. Consider: ACA subsidies if retiring early.\"\n    },\n    \n    # ==================== RISK MANAGEMENT (10 examples) ====================\n    {\n        \"instruction\": \"How much risk should I take?\",\n        \"input\": \"Age: 35, Time horizon: 30 years\",\n        \"output\": \"Risk capacity (35 years old, 30-year horizon): HIGH - Long time to recover from downturns. Rule of thumb: Stock allocation = 110 - Age = 75% stocks. Aggressive: 85-90% stocks. Moderate: 70-80% stocks. Conservative: 60-70% stocks. Risk tolerance test: Could you stomach 40% portfolio decline without selling? If yes → aggressive. If no → moderate. Recommendation: 80% stocks (60% US, 20% international), 20% bonds. Gradually reduce equity by 1% annually after age 40. Most 35-year-olds are too conservative.\"\n    },\n    {\n        \"instruction\": \"What is volatility?\",\n        \"input\": \"\",\n        \"output\": \"Volatility: Degree of price fluctuation, measured by standard deviation. S&P 500 volatility: ~15% annually. Meaning: ~2/3 of time, returns within 15% of average (if average is 10%, expect -5% to +25%). High volatility: Emerging markets, small-caps, individual stocks (30-50%). Low volatility: Bonds, utilities, large-caps (10-20%). VIX index: Market's fear gauge (under 15 = calm, over 30 = panic). Volatility ≠ risk: Temporary fluctuations acceptable for long-term investors. Only realize losses if you sell.\"\n    },\n    {\n        \"instruction\": \"How to protect portfolio in market downturn?\",\n        \"input\": \"\",\n        \"output\": \"Downturn protection strategies: BEFORE crash: Proper asset allocation (age-appropriate bonds), Rebalance regularly, Build 6-month emergency fund, Diversify globally, Avoid leverage. DURING crash: Don't panic sell (worst mistake), Rebalance (buy stocks if below target), Deploy cash reserves into quality names, Tax-loss harvest, Review but don't abandon plan. Avoid: Market timing, going 100% cash, buying inverse ETFs. Best protection: Time horizon (don't need money for 10+ years), emotional discipline, remembering markets always recover.\"\n    },\n    {\n        \"instruction\": \"What is stop-loss strategy?\",\n        \"input\": \"\",\n        \"output\": \"Stop-loss: Automatic sell order triggered at predetermined price. Example: Buy stock at $100, set stop-loss at $90 (10% loss limit). Pros: Limits losses, removes emotion, protects profits (trailing stop). Cons: Can be triggered by temporary volatility, locks in losses, miss recovery. Usage: Active traders (essential), swing trading, speculative positions. NOT recommended for: Long-term investors (volatility is normal), index funds, inside retirement accounts. Alternative: Mental stops (review quarterly), position sizing (limit each stock to 5% of portfolio).\"\n    },\n    {\n        \"instruction\": \"Should I hedge my portfolio?\",\n        \"input\": \"\",\n        \"output\": \"Portfolio hedging: Using derivatives to reduce risk. Common hedges: Put options (downside protection), Inverse ETFs (profit from decline), Gold (crisis hedge), Bonds (negative correlation). Costs: Hedges cost money (drag on returns), complex, timing difficult. Reality: Most individual investors should NOT hedge. Reasons: Costs exceed benefits long-term, complexity risk, natural hedging better (bonds, diversification). Exception: Hedge if: Concentrated position (company stock), near retirement with large portfolio, specific event risk. Better approach: Proper asset allocation, rebalancing, emergency fund.\"\n    },\n    {\n        \"instruction\": \"What is position sizing?\",\n        \"input\": \"\",\n        \"output\": \"Position sizing: Determining how much to allocate to each investment. Guidelines: Individual stocks: 3-5% of portfolio max (15-30 total positions). High-risk/speculative: 1-2% max. Core holdings: 5-10% acceptable. Index funds: Can be 100%. Concentration risk: Don't let any position exceed 10% (rebalance). New positions: Start small (2-3%), add if thesis plays out. Conviction-weighting: Higher confidence = larger position (but respect limits). Most common mistake: Over-concentrating in winners or employer stock. Diversification protects against unknowns.\"\n    },\n    {\n        \"instruction\": \"What is drawdown?\",\n        \"input\": \"\",\n        \"output\": \"Drawdown: Peak-to-trough decline before new high. Example: Portfolio grows $100k → $150k → drops to $120k = 20% drawdown. Historical drawdowns: S&P 500 averages 14% intra-year decline annually, 10%+ drawdowns every 1-2 years, 20%+ every 5-7 years, 40%+ every 15-20 years (2000, 2008, 2020). Recovery time: 10% correction: 4 months average, 20% bear market: 2 years, 50% crash: 5+ years. Implication: Expect regular declines, don't sell, have cash cushion, drawdowns are buying opportunities for long-term investors.\"\n    },\n    {\n        \"instruction\": \"How much should be in cash?\",\n        \"input\": \"\",\n        \"output\": \"Cash allocation: Emergency fund: 3-6 months expenses (separate from investments). Investment portfolio: 0-5% cash for: rebalancing opportunities, psychological comfort, tactical deployment. More cash if: Near retirement (1-2 years expenses), expecting large purchase, risk-averse. Less cash if: Young, aggressive investor, long time horizon. Common mistake: Holding too much cash (loses to inflation, opportunity cost). Recommendations: Park in high-yield savings (4-5%), money market funds, short-term Treasuries. Don't use checking account (no interest). Rebalance to target allocation quarterly.\"\n    },\n    {\n        \"instruction\": \"What is maximum drawdown?\",\n        \"input\": \"\",\n        \"output\": \"Maximum drawdown: Largest peak-to-trough decline over specific period. Historical examples: S&P 500: -57% (2007-2009), -49% (2000-2002), -34% (2020), -20% (2022). Important: Drawdowns are NORMAL. Recovery: -20% requires +25% gain to break even, -50% requires +100% gain (why losses hurt more). Risk tolerance test: If portfolio drops 30%, would you: A) Panic sell → too aggressive, B) Hold → appropriate risk, C) Buy more → could be more aggressive. Age consideration: Younger can tolerate higher max drawdowns (time to recover). Use historical max drawdown to set portfolio risk level.\"\n    },\n    {\n        \"instruction\": \"Should I buy bonds for safety?\",\n        \"input\": \"Age: 30\",\n        \"output\": \"Bonds at age 30: Minimal allocation needed (10-20% max). Reasons: 30+ years until retirement (long recovery time), stocks outperform bonds long-term (10% vs 5%), inflation erodes bond returns, young investors can handle volatility. Bond allocation: Age 30: 10-20%, Age 40: 20-30%, Age 50: 30-40%, Age 60+: 40-60%. Bond types: Government bonds (safety), Investment-grade corporate (yield), I-Bonds (inflation protection). Don't: Over-allocate to bonds early (sacrifices growth). Do: Gradually increase bond allocation as you age. At 30, prioritize stock growth over bond safety.\"\n    },\n    \n    # ==================== MARKET CONCEPTS (10 examples) ====================\n    {\n        \"instruction\": \"What causes stock market crashes?\",\n        \"input\": \"\",\n        \"output\": \"Market crash causes: Valuation extremes (bubbles), Economic shocks (recessions, pandemic), Credit crises (2008 housing), Monetary policy (rapid rate hikes), Geopolitical events (wars), Psychological panic (feedback loops). Historical examples: 1929 (speculation), 1987 (program trading), 2000 (dot-com bubble), 2008 (credit crisis), 2020 (pandemic). Common pattern: Excessive optimism → overvaluation → trigger event → panic selling. Prevention impossible (part of capitalism). Protection: Diversification, appropriate risk level, don't use leverage, keep cash reserve. Remember: Crashes create opportunities for long-term investors.\"\n    },\n    {\n        \"instruction\": \"What is a bull market?\",\n        \"input\": \"\",\n        \"output\": \"Bull market: Sustained market rise (20%+ from lows), optimistic investor sentiment. Characteristics: Rising earnings, economic expansion, low unemployment, increasing valuations. Duration: Average 4-5 years (longest: 2009-2020, 11 years). Behavior: Greed increases, valuations expand, bad companies rally too, 'this time is different' mentality. Strategy in bull market: Stay invested (most gains in final phase), Rebalance (trim winners), Don't get euphoric (leads to mistakes), Take some profits if extremely overvalued. Warning signs: Extreme valuations (P/E >25), margin debt highs, excessive optimism. Bull markets climb wall of worry.\"\n    },\n    {\n        \"instruction\": \"What is a bear market?\",\n        \"input\": \"\",\n        \"output\": \"Bear market: Decline of 20%+ from peak, pessimistic sentiment. Average: -35% decline, 18-month duration. Causes: Recessions, overvaluation corrections, credit crises. Investor behavior: Fear dominates, selling accelerates, quality stocks sold indiscriminately. Strategy: Don't panic sell (biggest mistake), Deploy cash gradually (scale in), Rebalance (increase stock allocation), Tax-loss harvest, Focus on quality companies. Historical fact: ALL bear markets recovered (100% success rate). Average recovery time: 2 years. Best gains often in first months of recovery (miss if in cash). Bear markets are buying opportunities, not disasters.\"\n    },\n    {\n        \"instruction\": \"What is market correction?\",\n        \"input\": \"\",\n        \"output\": \"Market correction: Decline of 10-20% from recent peak. Frequency: Occurs ~once per year on average (very common). Duration: Typically 3-6 months. Causes: Profit-taking, valuation reset, economic concerns, geopolitical events. Distinction: Correction (10-20%) vs Bear market (20%+). Healthy: Corrections prevent bubbles, shake out weak hands, create buying opportunities. Investor response: Normal part of investing (don't panic), Review portfolio but don't sell, Deploy cash if available, Consider buying quality names at discount. Historical data: S&P 500 corrects 14% intra-year but still positive ~75% of years. Corrections ≠ crash.\"\n    },\n    {\n        \"instruction\": \"What is market capitalization?\",\n        \"input\": \"\",\n        \"output\": \"Market cap: Total value of company (Share Price × Shares Outstanding). Categories: Mega-cap (>$200B): Apple, Microsoft - stable, lower growth. Large-cap ($10-200B): Most S&P 500 - balanced. Mid-cap ($2-10B): Higher growth, moderate risk. Small-cap ($300M-$2B): Highest growth potential, highest risk. Micro-cap (<$300M): Very risky, illiquid. Portfolio allocation: Core should be large/mega-cap (70-80%), Mid-cap (10-20%), Small-cap (5-10%) for growth. Small-caps outperform long-term but with higher volatility. Market cap ≠ stock price (compare companies by market cap, not price).\"\n    },\n    {\n        \"instruction\": \"What is price discovery?\",\n        \"input\": \"\",\n        \"output\": \"Price discovery: Market process of determining asset value through supply/demand. Mechanisms: Buyers and sellers negotiate, order books match bids/asks, information incorporated instantly. Efficient markets: Prices reflect all available information rapidly. Factors affecting: Trading volume (higher = better discovery), Transparency, Number of participants, Information access. Poor price discovery: Illiquid markets, non-traded assets (real estate), private companies. Importance: Efficient capital allocation, fair valuations, investor confidence. Volatility is part of price discovery (market finding 'true' value). Continuous process during market hours.\"\n    },\n    {\n        \"instruction\": \"What are leading economic indicators?\",\n        \"input\": \"\",\n        \"output\": \"Leading indicators predict future economic activity: Stock market (forward-looking), Manufacturing orders (business demand), Building permits (construction activity), Consumer confidence (spending intentions), Yield curve (inversions predict recessions), Unemployment claims (labor market health), Money supply growth (liquidity). Leading vs Lagging: Leading: Predict future (6-12 months), Lagging: Confirm past (unemployment, GDP). Use: Identify turning points (recession/recovery). Limitations: False signals possible, timing imperfect, external shocks unpredictable. For investors: Don't overreact to single indicator, watch multiple indicators, economic predictions are uncertain.\"\n    },\n    {\n        \"instruction\": \"What is the yield curve?\",\n        \"input\": \"\",\n        \"output\": \"Yield curve: Graph of bond yields across maturities (short-term to long-term). Normal curve: Long-term yields > short-term (upward sloping), indicates healthy economy. Flat curve: Long-term ≈ short-term, uncertainty about growth. Inverted curve: Short-term > long-term (downward sloping), RECESSION PREDICTOR (reliable). Historical: Inversions preceded 1990, 2001, 2008, 2020 recessions (12-18 months lag). Mechanism: Inverted curve → banks can't profit (borrow short/lend long), credit tightens → recession. Monitor: 10-year minus 2-year spread. Current inversion: Warning sign but not immediate crash. Respond: Review portfolio risk, maintain diversification.\"\n    },\n    {\n        \"instruction\": \"What is inflation?\",\n        \"input\": \"\",\n        \"output\": \"Inflation: Rising prices over time, reduces purchasing power. Measurement: CPI (Consumer Price Index), PCE (Fed's preferred). Target: Fed targets 2% annual inflation (healthy for economy). Causes: Demand-pull (too much demand), Cost-push (supply issues), Monetary (money supply growth). Historical: US averaged 3% (1913-2023), 1970s: 10%+ (stagflation), 2020s: 8%+ peak (pandemic stimulus). Impact on investors: Stocks: Generally positive hedge (companies raise prices), Bonds: Negative (fixed payments lose value), Real assets: Positive (real estate, commodities). Protection: TIPS (inflation-protected bonds), I-Bonds, Real estate, Commodities, stocks.\"\n    },\n    {\n        \"instruction\": \"What is quantitative easing?\",\n        \"input\": \"\",\n        \"output\": \"Quantitative Easing (QE): Central bank buys bonds to inject money into economy. Mechanism: Fed purchases Treasuries/MBS → increases bank reserves → lowers interest rates → stimulates borrowing/spending. History: 2008-2014 (QE1, QE2, QE3), 2020-2021 (COVID response). Effects: Lower yields, higher stock prices (TINA: There Is No Alternative), weaker currency, inflation risk. Criticism: Benefits wealthy (asset owners), creates bubbles, inequality. Tightening (QT): Reverse process (sell bonds, remove money). For investors: QE = bullish for stocks/bonds, QT = bearish. Fed policy drives markets significantly. Monitor Fed announcements closely.\"\n    },\n    \n    # ==================== TAX OPTIMIZATION (10 examples) ====================\n    {\n        \"instruction\": \"What is capital gains tax?\",\n        \"input\": \"\",\n        \"output\": \"Capital gains tax: Tax on investment profits. Short-term (<1 year): Taxed as ordinary income (10-37% brackets). Long-term (1+ year): Preferential rates - 0% (income <$44k single), 15% ($44-492k), 20% (>$492k). Strategy: Hold investments 1+ year for lower taxes. Example: $10k gain, sold at 11 months = $3,700 tax (37% bracket), sold at 13 months = $1,500 tax (15% rate) → $2,200 savings. Exceptions: Collectibles (28% max), Real estate (25% depreciation recapture). Planning: Harvest losses to offset gains, time sales for lower tax years, donate appreciated stock (avoid tax entirely).\"\n    },\n    {\n        \"instruction\": \"What is tax-loss harvesting?\",\n        \"input\": \"\",\n        \"output\": \"Tax-loss harvesting: Sell losing investments to offset capital gains. Process: Sell position at loss, immediately buy similar (not identical) security, use loss to offset gains/income ($3k max annually), carry forward unused losses. Example: Sell Stock A at $5k loss, buy similar Stock B, offset $5k gain elsewhere = $750-$1,000 tax savings. Rules: Wash sale rule (30 days), only in taxable accounts (not IRAs). Best timing: Year-end, market downturns, portfolio rebalancing. Benefit: Turn losses into tax deductions, improve after-tax returns by 0.5-1.5% annually. Most effective in volatile markets. Automated by robo-advisors.\"\n    },\n    {\n        \"instruction\": \"Should I donate stock or cash?\",\n        \"input\": \"Stock value: $10k, Cost basis: $5k, Tax bracket: 24%\",\n        \"output\": \"Donate appreciated stock: Advantages: Deduct full $10k market value, avoid $5k capital gain (save $750 in taxes), total benefit = $2,400 (deduction) + $750 (no cap gains) = $3,150. Donate cash: Deduct $10k but pay with after-tax dollars, benefit = $2,400. Clear winner: Donate stock (save $750 more). Requirements: Hold stock 1+ year, itemize deductions, donate to qualified charity. Process: Transfer shares directly (don't sell first). Best for: Large donations, highly appreciated positions, high-income earners. Caveat: AGI deduction limits (30% for stock vs 60% for cash). Use donor-advised funds for flexibility.\"\n    },\n    {\n        \"instruction\": \"What is asset location strategy?\",\n        \"input\": \"\",\n        \"output\": \"Asset location: Place tax-inefficient assets in tax-advantaged accounts, tax-efficient in taxable. Tax-inefficient (401k/IRA): Bonds (interest taxed as ordinary income), REITs (high dividends), Actively managed funds (high turnover), Commodities. Tax-efficient (taxable brokerage): Index funds (low turnover), Municipal bonds (tax-free interest), Growth stocks (defer taxes until sale), ETFs (structure minimizes distributions). Benefit: Save 0.1-0.75% annually through optimal location. Example: $1M portfolio, proper location saves $1k-$7.5k/year in taxes. Priority: Max contributions first, then optimize location. Don't let taxes dictate investment strategy (tail wagging dog).\"\n    },\n    {\n        \"instruction\": \"What are municipal bonds?\",\n        \"input\": \"Tax bracket: 32%\",\n        \"output\": \"Municipal bonds: State/local government debt, interest federally tax-exempt. Tax-equivalent yield calculation: Muni yield ÷ (1 - tax rate). Example: Muni pays 3%, taxable bond pays 5%, tax bracket 32% → Muni equivalent: 3% ÷ 0.68 = 4.41%. In this case, taxable bond better (5% > 4.41%). When to buy munis: High tax bracket (32%+), state with income tax (double tax-exempt), bonds held in taxable account. When to avoid: Low bracket (<24%), tax-advantaged account (IRA), need taxable losses. Types: General obligation (safer), Revenue bonds (specific projects). Default rate: Very low historically. Best for: High-income, conservative investors in taxable accounts.\"\n    },\n    {\n        \"instruction\": \"How to avoid wash sale rule?\",\n        \"input\": \"\",\n        \"output\": \"Wash sale rule: Can't claim loss if you buy 'substantially identical' security within 30 days before/after sale. Avoid wash sales: Wait 31 days before repurchasing (lose market exposure), Buy similar but not identical security (sell VTI, buy ITOT), Buy options on different underlying, Double position then wait (caution: increases risk). Example: Own Apple, down $5k. Sell Apple, immediately buy Microsoft (different company = allowed), claim $5k loss. Wash sale penalty: Loss disallowed (increases cost basis of new position - deferred, not lost). Applies: To both buys 30 days before AND after sale. Retirement accounts: Buying in IRA can trigger wash sale in taxable. Track carefully.\"\n    },\n    {\n        \"instruction\": \"When to convert to Roth IRA?\",\n        \"input\": \"\",\n        \"output\": \"Best times to convert Traditional IRA to Roth: Low-income years (between jobs, retired before RMDs, business loss), Market downturns (convert more shares at lower value), Tax law changes (if rates rising), Before RMDs start (age 73). Calculate: Marginal tax rate on conversion vs expected retirement rate. Convert when: Current rate < future rate, have cash to pay taxes (outside IRA), time for Roth to grow (5+ years). Don't convert when: High bracket now (>32%), insufficient time to benefit, can't afford taxes, RMDs won't push to higher bracket. Strategy: Partial conversions annually (fill up bracket), Convert in tranches ($50-100k/year), 'Roth ladder' for early retirees.\"\n    },\n    {\n        \"instruction\": \"What is the backdoor Roth IRA?\",\n        \"input\": \"Income: $200k (above Roth limit)\",\n        \"output\": \"Backdoor Roth IRA: Legal workaround for high earners to contribute to Roth. Process: 1) Contribute $6,500 to Traditional IRA (non-deductible), 2) Immediately convert to Roth IRA, 3) Pay taxes only on growth (minimal if immediate). Income limits: Direct Roth phaseout at $138-153k (single), $218-228k (married). Backdoor has NO income limits. Pro-rata rule: If you have existing Traditional IRA balances, conversions are partially taxable (mix of pre-tax and after-tax money). Solution: Roll existing Traditional IRA into 401(k) first (if allowed). Annual: Repeat every year. Benefits same as regular Roth: Tax-free growth, no RMDs. Essential strategy for high earners.\"\n    },\n    {\n        \"instruction\": \"What are qualified dividends?\",\n        \"input\": \"\",\n        \"output\": \"Qualified dividends: Taxed at long-term capital gains rates (0%, 15%, 20%) instead of ordinary income (10-37%). Requirements: US company or qualified foreign corporation, hold stock 60+ days during 121-day period around ex-dividend date. Ordinary (non-qualified) dividends: REITs, MLPs, some foreign stocks → taxed as ordinary income (higher). Tax impact: $10k qualified dividends at 15% = $1,500 tax. $10k ordinary at 32% = $3,200 tax → $1,700 difference. Strategy: Hold dividend stocks 60+ days (long enough for qualified treatment), prefer qualified dividend payers in taxable accounts, REITs in IRAs. Most blue-chip stocks pay qualified dividends. Check 1099-DIV form (shows which dividends are qualified).\"\n    },\n    {\n        \"instruction\": \"Should I max out 401(k) or taxable account?\",\n        \"input\": \"\",\n        \"output\": \"Priority order: 1) 401(k) to employer match (free money), 2) Max Roth IRA ($6,500), 3) Max 401(k) ($22,500), 4) HSA if eligible ($3,850 individual), 5) Taxable brokerage. 401(k) advantages: Tax-deferred growth, lower current taxes, forced discipline, higher contribution limits, creditor protection. Taxable advantages: No withdrawal penalties, flexibility (access anytime), tax-loss harvesting, capital gains rates, no RMDs. Max 401(k) if: High tax bracket (>24%), not maxing yet, employer match available. Use taxable if: Already maxing retirement accounts, need flexibility, early retirement (bridge to 59.5). Most people should max 401(k) before taxable investing.\"\n    }\n]\n\nfrom datasets import Dataset\ncustom_dataset = Dataset.from_list(custom_investment_data)\nprint(f\"✓ Custom investment dataset created: {len(custom_dataset)} examples\")\nprint(f\"  Purpose: Investment-specific reasoning and recommendations\")\n\nprint(\"\\n\" + \"=\" * 80)","metadata":{"id":"tny4je0AeQLl","outputId":"390479e1-5510-4180-8b31-3438d9502a42","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:49:59.114899Z","iopub.execute_input":"2025-10-30T13:49:59.115217Z","iopub.status.idle":"2025-10-30T13:50:07.709545Z","shell.execute_reply.started":"2025-10-30T13:49:59.115197Z","shell.execute_reply":"2025-10-30T13:50:07.708968Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nFINANCIAL SLM - Multi-Dataset Preparation\n================================================================================\n\n[1/4] Loading Finance Alpaca dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/831 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fc663f49b0545fa88b2c77befb90c65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Cleaned_date.json:   0%|          | 0.00/42.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3ad6adb9a484bf8a7d0af04eeb1f929"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/68912 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfd6d60bfbf04026b1d805026666e92c"}},"metadata":{}},{"name":"stdout","text":"✓ Finance Alpaca loaded: 68,912 examples\n  Purpose: Financial instruction-following and Q&A\n\n[2/4] Loading Financial PhraseBank dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/721 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bcfabd5641643e0bca699108516f92c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-138b53eb17a3e8(…):   0%|          | 0.00/268k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b33eae3c905547ca8093c5bf29daa349"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001-0876be41e(…):   0%|          | 0.00/68.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aee6610f35e64885b14801293ae02c75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001-41c7ea948573445(…):   0%|          | 0.00/82.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6e8db755a414305ae0cb5b25a7d325b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d58e486f204be594fa49620bd6ecb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/776 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bc6d2970ff4478aacb3a14cbcc5c930"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/970 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cb4416345af4cd591d8eecd1b4d7fe9"}},"metadata":{}},{"name":"stdout","text":"✓ Financial PhraseBank loaded: 3,100 examples\n  Purpose: Financial sentiment understanding (positive/negative/neutral)\n  Sentiment distribution: {2: 866, 0: 382, 1: 1852}\n\n[3/4] Loading General Alpaca dataset for English understanding...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8061774d00f4fb4a99d01e7507be77f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-a09b74b3ef9c3b(…):   0%|          | 0.00/24.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c7ec78a6b9a43e5a32bafe3c6182b88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73d4504fc48a4c5aa8477867358b85a7"}},"metadata":{}},{"name":"stdout","text":"✓ General Alpaca loaded: 20,000 examples\n  Purpose: General English instruction-following and reasoning\n\n[4/4] Creating custom investment recommendation dataset...\n✓ Custom investment dataset created: 100 examples\n  Purpose: Investment-specific reasoning and recommendations\n\n================================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Step 3: Format and Combine Datasets","metadata":{}},{"cell_type":"code","source":"print(\"\\n📝 Formatting datasets\")\n\ndef format_text(example):\n    \"\"\"Unified formatting function\"\"\"\n    text = f\"\"\"### Instruction:\n{example['instruction']}\n\n### Input:\n{example['input']}\n\n### Response:\n{example['output']}\n\n### End\"\"\"\n    return {\"text\": text}\n\n# Format Finance Alpaca\nalpaca_formatted = finance_alpaca['train'].map(format_text, desc=\"Formatting Finance Alpaca\")\nprint(f\"✓ Finance Alpaca formatted: {len(alpaca_formatted):,} examples\")\n\ndatasets_to_combine = [alpaca_formatted]\n\n# Format General Alpaca (for English understanding)\ngeneral_alpaca_formatted = general_alpaca_sample.map(format_text, desc=\"Formatting General Alpaca\")\nprint(f\"✓ General Alpaca formatted: {len(general_alpaca_formatted):,} examples\")\ndatasets_to_combine.append(general_alpaca_formatted)\n\n# Format Financial PhraseBank\ndef format_phrasebank(example):\n    sentiment_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n    sentiment = sentiment_map[example['label']]\n    \n    return {\n        \"instruction\": \"Analyze the sentiment of this financial statement from an investor perspective.\",\n        \"input\": example['sentence'],\n        \"output\": f\"The sentiment of this statement is {sentiment}. This suggests the news may have a {sentiment} influence on stock price from an investor's viewpoint.\"\n    }\n\nphrasebank_with_fields = financial_phrasebank['train'].map(format_phrasebank, desc=\"Converting PhraseBank\")\nphrasebank_formatted = phrasebank_with_fields.map(format_text, desc=\"Formatting PhraseBank\")\nprint(f\"✓ Financial PhraseBank formatted: {len(phrasebank_formatted):,} examples\")\ndatasets_to_combine.append(phrasebank_formatted)\n\n# Format Custom Investment Dataset\ncustom_formatted = custom_dataset.map(format_text, desc=\"Formatting Custom\")\nprint(f\"✓ Custom dataset formatted: {len(custom_formatted)} examples\")\ndatasets_to_combine.append(custom_formatted)\n\n# Remove extra columns (keep only 'text' column)\nfor i, ds in enumerate(datasets_to_combine):\n    datasets_to_combine[i] = ds.remove_columns(\n        [col for col in ds.column_names if col != 'text']\n    )\n\n# Concatenate all datasets\ncombined_dataset = concatenate_datasets(datasets_to_combine)\n\nprint(f\"\\n✓ Combined dataset size: {len(combined_dataset):,} examples\")\nprint(f\"\\n📊 Dataset Distribution:\")\nprint(f\"  Finance Alpaca:    {len(alpaca_formatted):>6,} ({len(alpaca_formatted)/len(combined_dataset)*100:.1f}%)\")\nprint(f\"  General Alpaca:    {len(general_alpaca_formatted):>6,} ({len(general_alpaca_formatted)/len(combined_dataset)*100:.1f}%)\")\nprint(f\"  Financial PhraseBank: {len(phrasebank_formatted):>6,} ({len(phrasebank_formatted)/len(combined_dataset)*100:.1f}%)\")\nprint(f\"  Custom Investment:    {len(custom_formatted):>6,} ({len(custom_formatted)/len(combined_dataset)*100:.1f}%)\")\n\nprint(\"\\n\" + \"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:50:07.710345Z","iopub.execute_input":"2025-10-30T13:50:07.710622Z","iopub.status.idle":"2025-10-30T13:50:13.128671Z","shell.execute_reply.started":"2025-10-30T13:50:07.710594Z","shell.execute_reply":"2025-10-30T13:50:13.127911Z"}},"outputs":[{"name":"stdout","text":"\n📝 Formatting datasets\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Formatting Finance Alpaca:   0%|          | 0/68912 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5592698c087042738aac109179b01323"}},"metadata":{}},{"name":"stdout","text":"✓ Finance Alpaca formatted: 68,912 examples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Formatting General Alpaca:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c024df9d474eadbe0c23387108ece4"}},"metadata":{}},{"name":"stdout","text":"✓ General Alpaca formatted: 20,000 examples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Converting PhraseBank:   0%|          | 0/3100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91b64dee0eea41f38ac4e5c3a093597a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Formatting PhraseBank:   0%|          | 0/3100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0863d74a67fc400db21d97b7ac959052"}},"metadata":{}},{"name":"stdout","text":"✓ Financial PhraseBank formatted: 3,100 examples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Formatting Custom:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e060eec361c4f9f867b1d75bd60ec11"}},"metadata":{}},{"name":"stdout","text":"✓ Custom dataset formatted: 100 examples\n\n✓ Combined dataset size: 92,112 examples\n\n📊 Dataset Distribution:\n  Finance Alpaca:    68,912 (74.8%)\n  General Alpaca:    20,000 (21.7%)\n  Financial PhraseBank:  3,100 (3.4%)\n  Custom Investment:       100 (0.1%)\n\n================================================================================\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Step 4: Create Train/Test Split","metadata":{}},{"cell_type":"code","source":"print(\"\\n✂️ Creating train/test split...\")\n\ndataset_split = combined_dataset.train_test_split(test_size=0.1, seed=42)\n\nprint(f\"✓ Splits created:\")\nprint(f\"  - Train: {len(dataset_split['train']):,} examples\")\nprint(f\"  - Test: {len(dataset_split['test']):,} examples\")\n\nprint(\"\\n\" + \"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:50:13.130659Z","iopub.execute_input":"2025-10-30T13:50:13.130933Z","iopub.status.idle":"2025-10-30T13:50:13.158895Z","shell.execute_reply.started":"2025-10-30T13:50:13.130915Z","shell.execute_reply":"2025-10-30T13:50:13.158196Z"}},"outputs":[{"name":"stdout","text":"\n✂️ Creating train/test split...\n✓ Splits created:\n  - Train: 82,900 examples\n  - Test: 9,212 examples\n\n================================================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Step 5: Tokenize the Combined Dataset","metadata":{"id":"vZIMnCMzglaf"}},{"cell_type":"code","source":"import tiktoken\nimport os\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport tiktoken\n\nprint(\"\\n⚙️ Tokenizing dataset...\")\n\n# Initialize GPT-2 tokenizer\nencoder = tiktoken.get_encoding(\"gpt2\")\nprint(f\"✓ Tokenizer loaded: {encoder.name}\")\nprint(f\"  Vocabulary size: {encoder.n_vocab:,}\")\n\ndef tokenize_example(example, encoder):\n    \"\"\"\n    Convert text string to token IDs.\n    Returns both the token IDs list and length for statistics.\n    \"\"\"\n    ids = encoder.encode_ordinary(example[\"text\"])\n    return {\"ids\": ids, \"len\": len(ids)}\n\nprint(\"\\nTokenizing train and test splits...\")\ntokenized = dataset_split.map(\n    tokenize_example,\n    fn_kwargs={\"encoder\": encoder},\n    remove_columns=[\"text\"],\n    desc=\"Tokenizing\",\n    num_proc=os.cpu_count(),\n)\n\nprint(f\"\\n✓ Tokenization complete!\")\nprint(f\"  Columns after tokenization: {tokenized['train'].column_names}\")\n\nprint(f\"\\n📋 Sample tokenized example:\")\nprint(f\"  First 20 token IDs: {tokenized['train'][0]['ids'][:20]}\")\nprint(f\"  Token count: {tokenized['train'][0]['len']}\")\n\ntrain_tokens = np.sum(tokenized['train']['len'], dtype=np.int64)\ntest_tokens = np.sum(tokenized['test']['len'], dtype=np.int64)\ntotal_tokens = train_tokens + test_tokens\n\nprint(f\"\\n📊 Token Statistics:\")\nprint(f\"  - Train tokens: {train_tokens:,}\")\nprint(f\"  - Test tokens: {test_tokens:,}\")\nprint(f\"  - Total tokens: {total_tokens:,}\")\nprint(f\"  - Avg tokens/example (train): {train_tokens / len(tokenized['train']):.1f}\")\nprint(f\"  - Avg tokens/example (test): {test_tokens / len(tokenized['test']):.1f}\")\nprint(f\"  - Max tokens in example: {max(tokenized['train']['len'])}\")\nprint(f\"  - Min tokens in example: {min(tokenized['train']['len'])}\")\n\nprint(\"\\n\" + \"=\" * 80)","metadata":{"id":"8PUoe7zngj1j","outputId":"bed53599-a167-46bf-ef37-a46e8bacfdf1","scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:50:13.159696Z","iopub.execute_input":"2025-10-30T13:50:13.160030Z","iopub.status.idle":"2025-10-30T13:50:25.430658Z","shell.execute_reply.started":"2025-10-30T13:50:13.160006Z","shell.execute_reply":"2025-10-30T13:50:25.429836Z"}},"outputs":[{"name":"stdout","text":"\n⚙️ Tokenizing dataset...\n✓ Tokenizer loaded: gpt2\n  Vocabulary size: 50,257\n\nTokenizing train and test splits...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing (num_proc=4):   0%|          | 0/82900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a2e7ecf7d624deeac3fc44020cd9bda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing (num_proc=4):   0%|          | 0/9212 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b60ae8976edf43c8a486baa6bf606a26"}},"metadata":{}},{"name":"stdout","text":"\n✓ Tokenization complete!\n  Columns after tokenization: ['ids', 'len']\n\n📋 Sample tokenized example:\n  First 20 token IDs: [21017, 46486, 25, 198, 16742, 262, 12867, 286, 8263, 257, 734, 422, 257, 3210, 6203, 286, 6740, 4116, 13, 198]\n  Token count: 56\n\n📊 Token Statistics:\n  - Train tokens: 10,351,006\n  - Test tokens: 1,139,892\n  - Total tokens: 11,490,898\n  - Avg tokens/example (train): 124.9\n  - Avg tokens/example (test): 123.7\n  - Max tokens in example: 3543\n  - Min tokens in example: 25\n\n================================================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Step 6: Load Tokenized Data Directly to RAM","metadata":{"id":"CvMEu0_xoq-B"}},{"cell_type":"code","source":"import torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice_type = 'cuda' if 'cuda' in device else 'cpu'\n\nprint(\"\\n💾 Loading tokenized data to RAM\")\n\ntrain_data = torch.cat([\n    torch.tensor(example['ids'], dtype=torch.long) \n    for example in tqdm(tokenized['train'], desc=\"Loading train\")\n])\nprint(f\"✓ Train data: {len(train_data):,} tokens ({len(train_data)*8/1024/1024:.2f} MB)\")\n\ntest_data = torch.cat([\n    torch.tensor(example['ids'], dtype=torch.long) \n    for example in tqdm(tokenized['test'], desc=\"Loading test\")\n])\nprint(f\"✓ Test data: {len(test_data):,} tokens ({len(test_data)*8/1024/1024:.2f} MB)\")\n\ndef get_batch(split, batch_size, block_size, device, device_type):\n    \"\"\"Fast in-memory batch generation - no disk I/O\"\"\"\n    data = train_data if split == 'train' else test_data\n    \n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n    \n    if device_type == 'cuda':\n        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n    else:\n        x, y = x.to(device), y.to(device)\n    \n    return x, y\n\nprint(\"✓ Data ready for training\")\nprint(\"=\" * 80)","metadata":{"id":"gFsxe997ocaa","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:50:25.431650Z","iopub.execute_input":"2025-10-30T13:50:25.432028Z","iopub.status.idle":"2025-10-30T13:50:35.165914Z","shell.execute_reply.started":"2025-10-30T13:50:25.432002Z","shell.execute_reply":"2025-10-30T13:50:35.164986Z"}},"outputs":[{"name":"stdout","text":"\n💾 Loading tokenized data to RAM\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading train:   0%|          | 0/82900 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6182594d96f0412ab5ae184d690cc388"}},"metadata":{}},{"name":"stdout","text":"✓ Train data: 10,351,006 tokens (78.97 MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading test:   0%|          | 0/9212 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c2a4a40b79c43e0a691f348c93de546"}},"metadata":{}},{"name":"stdout","text":"✓ Test data: 1,139,892 tokens (8.70 MB)\n✓ Data ready for training\n================================================================================\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport os\n\n# Detect available GPUs\nn_gpus = torch.cuda.device_count()\nprint(f\"🔍 Detected {n_gpus} GPUs\")\n\nif n_gpus > 1:\n    print(f\"🚀 MULTI-GPU MODE: Using {n_gpus} GPUs\")\n    device = 'cuda'\n    device_type = 'cuda'\n    is_multi_gpu = True\nelse:\n    print(f\"✓ SINGLE GPU MODE\")\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    device_type = 'cuda' if 'cuda' in device else 'cpu'\n    is_multi_gpu = False\n\nprint(f\"Device: {device}\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:50:35.166693Z","iopub.execute_input":"2025-10-30T13:50:35.166952Z","iopub.status.idle":"2025-10-30T13:50:35.173139Z","shell.execute_reply.started":"2025-10-30T13:50:35.166933Z","shell.execute_reply":"2025-10-30T13:50:35.172406Z"}},"outputs":[{"name":"stdout","text":"🔍 Detected 2 GPUs\n🚀 MULTI-GPU MODE: Using 2 GPUs\nDevice: cuda\n================================================================================\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Step 7: Defining the model","metadata":{"id":"2XudTjppqlBe"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom dataclasses import dataclass\n\nclass LayerNorm(nn.Module):\n    \"\"\"LayerNorm with optional bias\"\"\"\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n    \n    def forward(self, x):\n        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"Multi-head causal self-attention\"\"\"\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        \n        # Q, K, V projections for all heads in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # Output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # Regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        \n        # Flash attention support (much faster)\n        self.flash = hasattr(F, 'scaled_dot_product_attention')\n        if not self.flash:\n            # Causal mask for non-flash attention\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size()  # batch, sequence length, embedding dim\n        \n        # Calculate Q, K, V for all heads in batch\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n\n        # Efficient attention\n        if self.flash:\n            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, \n                                                dropout_p=self.attn_dropout.p if self.training else 0.0, \n                                                is_causal=True)\n        else:\n            # Manual attention computation\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nclass MLP(nn.Module):\n    \"\"\"Feed-forward network\"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu = nn.GELU()\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n\nclass Block(nn.Module):\n    \"\"\"Transformer block: attention + MLP with residual connections\"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.ln1 = LayerNorm(config.n_embd, config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln2 = LayerNorm(config.n_embd, config.bias)\n        self.mlp = MLP(config)\n    \n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n@dataclass\nclass GPTConfig:\n    \"\"\"GPT model configuration\"\"\"\n    block_size: int       # Max sequence length\n    vocab_size: int       # Vocabulary size\n    n_layer: int          # Number of transformer blocks\n    n_head: int           # Number of attention heads\n    n_embd: int           # Embedding dimension\n    dropout: float = 0.0  # Dropout rate\n    bias: bool = True     # Use bias in linear layers\n\nclass GPT(nn.Module):\n    \"\"\"GPT Language Model\"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        \n        self.transformer = nn.ModuleDict(dict(\n            wte=nn.Embedding(config.vocab_size, config.n_embd),  # Token embeddings\n            wpe=nn.Embedding(config.block_size, config.n_embd),  # Position embeddings\n            drop=nn.Dropout(config.dropout),\n            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f=LayerNorm(config.n_embd, config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        \n        # Weight tying: share embeddings with output layer\n        self.transformer.wte.weight = self.lm_head.weight\n\n        # Initialize weights\n        self.apply(self._init_weights)\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size\n        \n        pos = torch.arange(0, t, dtype=torch.long, device=device)\n        \n        # Forward pass\n        tok_emb = self.transformer.wte(idx)\n        pos_emb = self.transformer.wpe(pos)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        \n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            # Training mode: compute loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n            return logits, loss\n        else:\n            # Inference mode: only compute last token\n            logits = self.lm_head(x[:, [-1], :])\n            return logits, None\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Generate new tokens autoregressively.\n        \n        Args:\n            idx: Input token indices [B, T]\n            max_new_tokens: Number of tokens to generate\n            temperature: Sampling temperature (higher = more random)\n            top_k: If set, only sample from top k most likely tokens\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # Crop context if needed\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            # Get predictions\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :] / temperature\n            \n            # Optional top-k filtering\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            \n            # Sample from distribution\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        \n        return idx","metadata":{"id":"HJEpsVQOqnFg","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:50:35.173905Z","iopub.execute_input":"2025-10-30T13:50:35.174136Z","iopub.status.idle":"2025-10-30T13:50:35.203539Z","shell.execute_reply.started":"2025-10-30T13:50:35.174113Z","shell.execute_reply":"2025-10-30T13:50:35.202807Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Create model configuration for Financial SLM\nconfig = GPTConfig(\n    vocab_size=50257,\n    block_size=256,\n    n_layer=10,\n    n_head=12,\n    n_embd=768,\n    dropout=0.2,\n    bias=True\n)\n\nmodel = GPT(config)\n\n# Move to device\nmodel = model.to(device)\n\n# Wrap with DDP for multi-GPU\nif is_multi_gpu:\n    from torch.nn import DataParallel\n    model = DataParallel(model)\n    print(f\"✓ Model parallelized across {n_gpus} GPUs\")\n    \n    # Adjust batch size\n    batch_size = 8 * n_gpus  # Each GPU gets 8\n    print(f\"✓ Batch size: {batch_size} ({8} per GPU × {n_gpus} GPUs)\")\n\nn_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\n✓ Model ready for fine-tuning\")\nprint(f\"Parameters: {n_params/1e6:.2f}M\")\nprint(f\"  Context window: {config.block_size} tokens\")\nprint(f\"  Embedding dimension: {config.n_embd}\")\nprint(f\"  Transformer blocks: {config.n_layer}\")\nprint(f\"  Attention heads: {config.n_head}\")\n\nprint(\"\\n\" + \"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:50:35.204976Z","iopub.execute_input":"2025-10-30T13:50:35.205268Z","iopub.status.idle":"2025-10-30T13:50:37.857730Z","shell.execute_reply.started":"2025-10-30T13:50:35.205243Z","shell.execute_reply":"2025-10-30T13:50:37.857083Z"}},"outputs":[{"name":"stdout","text":"✓ Model parallelized across 2 GPUs\n✓ Batch size: 16 (8 per GPU × 2 GPUs)\n\n✓ Model ready for fine-tuning\nParameters: 109.67M\n  Context window: 256 tokens\n  Embedding dimension: 768\n  Transformer blocks: 10\n  Attention heads: 12\n\n================================================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Step 8: Training Configuration","metadata":{"id":"42BZ49dhyQdr"}},{"cell_type":"code","source":"import torch\nfrom contextlib import nullcontext\nfrom torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR\n\nprint(\"=\" * 80)\nprint(\"FINANCIAL SLM - Training Configuration\")\nprint(\"=\" * 80)\n\n# Training hyperparameters\nlearning_rate = 3e-4\nmax_iters = 10000\nwarmup_steps = 500\nmin_lr = 1e-5\neval_interval = 100\nbatch_size = 8\nblock_size = 256\ngradient_accumulation_steps = 4\n\n# Mixed precision setup\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\n# Set seeds for reproducibility\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n\nprint(f\"✓ Training configuration set:\")\nprint(f\"  Device: {device} ({device_type})\")\nprint(f\"  Mixed precision: {dtype}\")\nprint(f\"  Learning rate: {learning_rate}\")\nprint(f\"  Max iterations: {max_iters:,}\")\nprint(f\"  Batch size: {batch_size} (effective: {batch_size * gradient_accumulation_steps})\")\nprint(f\"  Block size: {block_size}\")\nprint(f\"  Eval interval: {eval_interval}\")","metadata":{"id":"aPFfA3k_yPuk","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:50:37.858436Z","iopub.execute_input":"2025-10-30T13:50:37.858616Z","iopub.status.idle":"2025-10-30T13:50:37.868531Z","shell.execute_reply.started":"2025-10-30T13:50:37.858602Z","shell.execute_reply":"2025-10-30T13:50:37.867918Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nFINANCIAL SLM - Training Configuration\n================================================================================\n✓ Training configuration set:\n  Device: cuda (cuda)\n  Mixed precision: bfloat16\n  Learning rate: 0.0003\n  Max iterations: 10,000\n  Batch size: 8 (effective: 32)\n  Block size: 256\n  Eval interval: 100\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Step 9: Loss Estimation Function","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss(model, eval_iters=200):\n    \"\"\"\n    Estimate model loss on train and test sets.\n    \n    Args:\n        model: The GPT model\n        eval_iters: Number of batches to evaluate\n    \n    Returns:\n        Dictionary with 'train' and 'val' losses\n    \"\"\"\n    out = {}\n    model.eval()\n    \n    for split in ['train', 'val']:  # 'val' uses test.bin\n        losses = torch.zeros(eval_iters)\n        \n        for k in range(eval_iters):\n            X, Y = get_batch(split, batch_size, block_size, device, device_type)\n            \n            with ctx:\n                logits, loss = model(X, Y)\n                if is_multi_gpu:\n                    loss = loss.mean()\n            losses[k] = loss.item()\n        \n        out[split] = losses.mean()\n    \n    model.train()\n    return out\n\nprint(\"✓ Loss estimation function defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:50:37.869156Z","iopub.execute_input":"2025-10-30T13:50:37.869399Z","iopub.status.idle":"2025-10-30T13:50:38.300303Z","shell.execute_reply.started":"2025-10-30T13:50:37.869381Z","shell.execute_reply":"2025-10-30T13:50:38.299598Z"}},"outputs":[{"name":"stdout","text":"✓ Loss estimation function defined\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Step 10: Optimizer and Scheduler Setup","metadata":{"id":"yC3YXHl50CBF"}},{"cell_type":"code","source":"# Move model to device\nmodel = model.to(device)\nprint(f\"✓ Model moved to {device}\")\n\n# Optimizer with weight decay\noptimizer = torch.optim.AdamW(\n    model.parameters(), \n    lr=learning_rate, \n    betas=(0.9, 0.95),\n    weight_decay=0.1,      # L2 regularization\n    eps=1e-9\n)\n\n# Learning rate scheduler: warmup + cosine annealing\nscheduler_warmup = LinearLR(optimizer, start_factor=0.1, total_iters=warmup_steps)\nscheduler_main = CosineAnnealingLR(optimizer, T_max=max_iters-warmup_steps, eta_min=min_lr)\nscheduler = SequentialLR(optimizer, \n                        schedulers=[scheduler_warmup, scheduler_main], \n                        milestones=[warmup_steps])\n\n# Gradient scaler for mixed precision\nscaler = torch.amp.GradScaler(enabled=(dtype == 'float16'))\n\nprint(\"✓ Optimizer and scheduler configured\")\nprint(f\"  Optimizer: AdamW (lr={learning_rate}, wd=0.1)\")\nprint(f\"  Scheduler: Warmup ({warmup_steps}) + Cosine annealing\")\nprint(f\"  Mixed precision: {dtype}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:50:38.301291Z","iopub.execute_input":"2025-10-30T13:50:38.301602Z","iopub.status.idle":"2025-10-30T13:50:40.729801Z","shell.execute_reply.started":"2025-10-30T13:50:38.301577Z","shell.execute_reply":"2025-10-30T13:50:40.729137Z"}},"outputs":[{"name":"stdout","text":"✓ Model moved to cuda\n✓ Optimizer and scheduler configured\n  Optimizer: AdamW (lr=0.0003, wd=0.1)\n  Scheduler: Warmup (500) + Cosine annealing\n  Mixed precision: bfloat16\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Step 11.1 Training the model structure of English","metadata":{"execution":{"iopub.status.busy":"2025-10-30T07:51:48.994437Z","iopub.execute_input":"2025-10-30T07:51:48.994630Z","iopub.status.idle":"2025-10-30T08:41:33.086982Z","shell.execute_reply.started":"2025-10-30T07:51:48.994608Z","shell.execute_reply":"2025-10-30T08:41:33.086250Z"}}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"PHASE 1: GENERAL ENGLISH LANGUAGE TRAINING\")\nprint(\"=\" * 80)\nprint(\"Teaching model English structure with general text before financial data\")\n\n# Load general English dataset\nfrom datasets import load_dataset\n\nprint(\"\\n📚 Loading general English dataset...\")\nenglish_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:15000]\")\nprint(f\"✓ Loaded {len(english_dataset):,} general English examples\")\n\n# Tokenize English data\nprint(\"\\n⚙️ Tokenizing English data...\")\n\ndef tokenize_english(example, encoder):\n    \"\"\"Tokenize general text\"\"\"\n    ids = encoder.encode_ordinary(example[\"text\"])\n    return {\"ids\": ids, \"len\": len(ids)}\n\nenglish_tokenized = english_dataset.map(\n    tokenize_english,\n    fn_kwargs={\"encoder\": encoder},\n    remove_columns=[\"text\"],\n    desc=\"Tokenizing English\",\n    num_proc=os.cpu_count(),\n)\n\n# Create English training data\nenglish_train_data = torch.cat([\n    torch.tensor(example['ids'], dtype=torch.long) \n    for example in tqdm(english_tokenized, desc=\"Loading English data\")\n    if len(example['ids']) > 10  # Filter very short examples\n])\n\nprint(f\"✓ English training data: {len(english_train_data):,} tokens\")\n\n# Temporary get_batch function for English data\ndef get_batch_english(batch_size, block_size):\n    \"\"\"Get batch from English data\"\"\"\n    ix = torch.randint(len(english_train_data) - block_size, (batch_size,))\n    x = torch.stack([english_train_data[i:i+block_size] for i in ix])\n    y = torch.stack([english_train_data[i+1:i+1+block_size] for i in ix])\n    \n    if device_type == 'cuda':\n        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n    else:\n        x, y = x.to(device), y.to(device)\n    \n    return x, y\n\n# Phase 1 Training Configuration\nphase1_iters = 10000          # English training iterations\nphase1_lr = 2e-5\nphase1_eval_interval = 200\n\nprint(f\"\\n⚙️ Phase 1 Training Configuration:\")\nprint(f\"  Iterations: {phase1_iters:,}\")\nprint(f\"  Learning rate: {phase1_lr}\")\nprint(f\"  Purpose: Learn English grammar, vocabulary, sentence structure\")\n\n# Create Phase 1 optimizer\nphase1_optimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=phase1_lr,\n    betas=(0.89, 0.92),\n    weight_decay=0.14,\n    eps=1e-9\n)\n\n# Phase 1 scheduler\nphase1_scheduler = CosineAnnealingLR(phase1_optimizer, T_max=phase1_iters, eta_min=1e-5)\nphase1_scaler = torch.amp.GradScaler(enabled=(dtype == 'float16'))\n\n# Phase 1 Training Loop\nprint(f\"\\n🚀 Starting Phase 1 Training...\")\nmodel.train()\n\nphase1_losses = []\n\nfor iter_num in tqdm(range(phase1_iters), desc=\"Phase 1: English Training\"):\n    \n    # Periodic evaluation\n    if iter_num % phase1_eval_interval == 0:\n        model.eval()\n        eval_losses = []\n        for _ in range(500):  # Quick eval\n            X, Y = get_batch_english(batch_size, block_size)\n            with ctx:\n                with torch.no_grad():\n                    _, loss = model(X, Y)\n                    \n                    # ✅ FIX: Handle multi-GPU loss\n                    if is_multi_gpu:\n                        loss = loss.mean()  # Average across GPUs\n                    \n                    eval_losses.append(loss.item())\n        \n        avg_loss = sum(eval_losses) / len(eval_losses)\n        phase1_losses.append(avg_loss)\n        print(f\"\\n   Step {iter_num}: Loss {avg_loss:.4f}\")\n        model.train()\n    \n    # Training step\n    X, y = get_batch_english(batch_size, block_size)\n    \n    with ctx:        \n        _, loss = model(X, y)\n        if is_multi_gpu:\n            loss = loss.mean()\n\n        loss = loss / gradient_accumulation_steps\n    \n    phase1_scaler.scale(loss).backward()\n    \n    # Optimizer step\n    if ((iter_num + 1) % gradient_accumulation_steps == 0) or (iter_num + 1 == phase1_iters):\n        phase1_scaler.unscale_(phase1_optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        phase1_scaler.step(phase1_optimizer)\n        phase1_scaler.update()\n        phase1_optimizer.zero_grad(set_to_none=True)\n        phase1_scheduler.step()\n\n# Phase 1 complete\nprint(\"\\n✓ Phase 1 Complete!\")\nprint(f\"  Initial English loss: {phase1_losses[0]:.4f}\")\nprint(f\"  Final English loss: {phase1_losses[-1]:.4f}\")\nprint(f\"  Improvement: {phase1_losses[0] - phase1_losses[-1]:.4f}\")\n\n# Clean up English data to free memory\ndel english_dataset, english_tokenized, english_train_data\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✓ Model now understands English structure!\")\nprint(\"  Proceeding to Phase 2: Financial domain training...\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:50:40.732101Z","iopub.execute_input":"2025-10-30T13:50:40.732590Z","iopub.status.idle":"2025-10-30T14:48:38.793572Z","shell.execute_reply.started":"2025-10-30T13:50:40.732571Z","shell.execute_reply":"2025-10-30T14:48:38.792932Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nPHASE 1: GENERAL ENGLISH LANGUAGE TRAINING\n================================================================================\nTeaching model English structure with general text before financial data\n\n📚 Loading general English dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff212c07d7694cb5a07601d170dc8bdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffe5c8cc8f6b45abb58ad59ad2a51e30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f41873d6acb74944847223f733042894"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd154b52a4b34597a8c442af7556752b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6661dfcee0a41a08f1192204e702d17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc06da15804c4d5db712e6ef64a52a66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45cb2e1985444bc4b22cd676d7f6540a"}},"metadata":{}},{"name":"stdout","text":"✓ Loaded 15,000 general English examples\n\n⚙️ Tokenizing English data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing English (num_proc=4):   0%|          | 0/15000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfc02b05acf1406da3a5096282ebaed0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading English data:   0%|          | 0/15000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89115ff332bb4974879e135c8635e1be"}},"metadata":{}},{"name":"stdout","text":"✓ English training data: 945,582 tokens\n\n⚙️ Phase 1 Training Configuration:\n  Iterations: 10,000\n  Learning rate: 2e-05\n  Purpose: Learn English grammar, vocabulary, sentence structure\n\n🚀 Starting Phase 1 Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Phase 1: English Training:   0%|          | 0/10000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"127c1b5534554fcf9d51df2fe2faaeeb"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n   Step 0: Loss 10.9708\n\n   Step 200: Loss 8.8011\n\n   Step 400: Loss 8.3919\n\n   Step 600: Loss 8.0490\n\n   Step 800: Loss 7.7450\n\n   Step 1000: Loss 7.4941\n\n   Step 1200: Loss 7.2944\n\n   Step 1400: Loss 7.1408\n\n   Step 1600: Loss 7.0295\n\n   Step 1800: Loss 6.9430\n\n   Step 2000: Loss 6.8808\n\n   Step 2200: Loss 6.8335\n\n   Step 2400: Loss 6.7902\n\n   Step 2600: Loss 6.7678\n\n   Step 2800: Loss 6.7423\n\n   Step 3000: Loss 6.7317\n\n   Step 3200: Loss 6.7149\n\n   Step 3400: Loss 6.6989\n\n   Step 3600: Loss 6.6982\n\n   Step 3800: Loss 6.6761\n\n   Step 4000: Loss 6.6713\n\n   Step 4200: Loss 6.6907\n\n   Step 4400: Loss 6.6682\n\n   Step 4600: Loss 6.6683\n\n   Step 4800: Loss 6.6737\n\n   Step 5000: Loss 6.6746\n\n   Step 5200: Loss 6.6805\n\n   Step 5400: Loss 6.6954\n\n   Step 5600: Loss 6.6935\n\n   Step 5800: Loss 6.7146\n\n   Step 6000: Loss 6.7034\n\n   Step 6200: Loss 6.7140\n\n   Step 6400: Loss 6.7233\n\n   Step 6600: Loss 6.7284\n\n   Step 6800: Loss 6.7392\n\n   Step 7000: Loss 6.7347\n\n   Step 7200: Loss 6.7580\n\n   Step 7400: Loss 6.7843\n\n   Step 7600: Loss 6.7804\n\n   Step 7800: Loss 6.7908\n\n   Step 8000: Loss 6.7783\n\n   Step 8200: Loss 6.7896\n\n   Step 8400: Loss 6.8278\n\n   Step 8600: Loss 6.8330\n\n   Step 8800: Loss 6.8391\n\n   Step 9000: Loss 6.8386\n\n   Step 9200: Loss 6.8878\n\n   Step 9400: Loss 6.8884\n\n   Step 9600: Loss 6.9188\n\n   Step 9800: Loss 6.8962\n\n✓ Phase 1 Complete!\n  Initial English loss: 10.9708\n  Final English loss: 6.8962\n  Improvement: 4.0746\n\n================================================================================\n✓ Model now understands English structure!\n  Proceeding to Phase 2: Financial domain training...\n================================================================================\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Step 11.2: Financial Training Loop","metadata":{"id":"B_ZSQexo1IxE"}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"PHASE 2: FINANCIAL DOMAIN SPECIALIZATION\")\nprint(\"=\" * 80)\nprint(\"Fine-tuning on financial data (model already knows English)\")\n\n# Training tracking\nbest_val_loss = float('inf')\nbest_model_path = './best_financial_slm.pt'\ntrain_losses, val_losses = [], []\nsteps_list = []\n\n# Phase 2 configuration (lower LR since model already knows English)\nlearning_rate = 5e-5\nmax_iters = 4000\n\n# Recreate optimizer for Phase 2 (lower learning rate)\noptimizer = torch.optim.AdamW(\n    model.parameters(), \n    lr=learning_rate,\n    betas=(0.9, 0.95),\n    weight_decay=0.1,\n    eps=1e-9\n)\n\n# Phase 2 scheduler\nscheduler_warmup = LinearLR(optimizer, start_factor=0.1, total_iters=warmup_steps)\nscheduler_main = CosineAnnealingLR(optimizer, T_max=max_iters-warmup_steps, eta_min=min_lr)\nscheduler = SequentialLR(optimizer, \n                        schedulers=[scheduler_warmup, scheduler_main], \n                        milestones=[warmup_steps])\n\nscaler = torch.amp.GradScaler(enabled=(dtype == 'float16'))\n\nprint(f\"\\n⚙️ Phase 2 Configuration:\")\nprint(f\"  Learning rate: {learning_rate} (lower for fine-tuning)\")\nprint(f\"  Max iterations: {max_iters:,}\")\nprint(f\"  Purpose: Learn financial concepts, terminology, analysis\")\n\n# Initial evaluation\nprint(\"\\n📊 Initial evaluation (on financial data)...\")\ninitial_losses = estimate_loss(model, eval_iters=100)\nprint(f\"Initial losses - Train: {initial_losses['train']:.4f}, Val: {initial_losses['val']:.4f}\")\n\n# Record initial point\ntrain_losses.append(initial_losses['train'])\nval_losses.append(initial_losses['val'])\nsteps_list.append(0)\n\nmodel.train()\n\n# Phase 2 Training loop\nfor iter_num in tqdm(range(max_iters), desc=\"Phase 2: Financial Training\"):\n    \n    # Evaluation\n    if iter_num % eval_interval == 0 and iter_num > 0:\n        losses = estimate_loss(model, eval_iters=400)\n        \n        print(f\"\\n📈 Step {iter_num:,}\")\n        print(f\"   Train loss: {losses['train']:.4f}\")\n        print(f\"   Val loss: {losses['val']:.4f}\")\n        print(f\"   Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n        \n        train_losses.append(losses['train'])\n        val_losses.append(losses['val'])\n        steps_list.append(iter_num)\n        \n        if losses['val'] < best_val_loss:\n            best_val_loss = losses['val']\n            \n            # Unwrap DataParallel before saving\n            model_to_save = model.module if is_multi_gpu else model\n            \n            checkpoint = {\n                'model_state_dict': model_to_save.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'config': config,\n                'iter_num': iter_num,\n                'val_loss': best_val_loss,\n                'train_loss': losses['train'],\n                'phase': 'financial',\n            }\n            torch.save(checkpoint, best_model_path)\n            print(f\"   💾 New best model saved! (val_loss: {best_val_loss:.4f})\")\n    \n    X, y = get_batch('train', batch_size, block_size, device, device_type)\n    \n    with ctx:\n        logits, loss = model(X, y)\n        \n        if is_multi_gpu:\n            loss = loss.mean()\n        \n        loss = loss / gradient_accumulation_steps\n    \n    scaler.scale(loss).backward()\n    \n    if ((iter_num + 1) % gradient_accumulation_steps == 0) or (iter_num + 1 == max_iters):\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad(set_to_none=True)\n        scheduler.step()\n        \n# Final evaluation\nprint(\"\\n\" + \"=\" * 80)\nprint(\"PHASE 2 COMPLETE!\")\nprint(\"=\" * 80)\n\nfinal_losses = estimate_loss(model, eval_iters=200)\nprint(f\"\\n📊 Final Results:\")\nprint(f\"   Best validation loss: {best_val_loss:.4f}\")\nprint(f\"   Final train loss: {final_losses['train']:.4f}\")\nprint(f\"   Final val loss: {final_losses['val']:.4f}\")\nprint(f\"   Model saved to: {best_model_path}\")\n\nprint(f\"\\n🎯 Two-Phase Training Complete!\")\nprint(f\"   Phase 1: English structure learned\")\nprint(f\"   Phase 2: Financial knowledge added\")\nprint(f\"   Result: Model understands English AND finance\")","metadata":{"id":"fevOMaXN1H6c","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:48:38.794584Z","iopub.execute_input":"2025-10-30T14:48:38.794831Z","iopub.status.idle":"2025-10-30T15:40:17.273565Z","shell.execute_reply.started":"2025-10-30T14:48:38.794812Z","shell.execute_reply":"2025-10-30T15:40:17.272730Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nPHASE 2: FINANCIAL DOMAIN SPECIALIZATION\n================================================================================\nFine-tuning on financial data (model already knows English)\n\n⚙️ Phase 2 Configuration:\n  Learning rate: 5e-05 (lower for fine-tuning)\n  Max iterations: 4,000\n  Purpose: Learn financial concepts, terminology, analysis\n\n📊 Initial evaluation (on financial data)...\nInitial losses - Train: 12.0570, Val: 12.0968\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Phase 2: Financial Training:   0%|          | 0/4000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"187930a09e3542ba963cffdfbe5b1fe8"}},"metadata":{}},{"name":"stdout","text":"\n📈 Step 100\n   Train loss: 9.2069\n   Val loss: 9.2174\n   Learning rate: 0.000007\n   💾 New best model saved! (val_loss: 9.2174)\n\n📈 Step 200\n   Train loss: 8.5781\n   Val loss: 8.6015\n   Learning rate: 0.000010\n   💾 New best model saved! (val_loss: 8.6015)\n\n📈 Step 300\n   Train loss: 7.9791\n   Val loss: 7.9902\n   Learning rate: 0.000012\n   💾 New best model saved! (val_loss: 7.9902)\n\n📈 Step 400\n   Train loss: 7.6869\n   Val loss: 7.6973\n   Learning rate: 0.000014\n   💾 New best model saved! (val_loss: 7.6973)\n\n📈 Step 500\n   Train loss: 7.6107\n   Val loss: 7.5788\n   Learning rate: 0.000016\n   💾 New best model saved! (val_loss: 7.5788)\n\n📈 Step 600\n   Train loss: 7.4659\n   Val loss: 7.4716\n   Learning rate: 0.000018\n   💾 New best model saved! (val_loss: 7.4716)\n\n📈 Step 700\n   Train loss: 7.4121\n   Val loss: 7.4290\n   Learning rate: 0.000021\n   💾 New best model saved! (val_loss: 7.4290)\n\n📈 Step 800\n   Train loss: 7.3524\n   Val loss: 7.3315\n   Learning rate: 0.000023\n   💾 New best model saved! (val_loss: 7.3315)\n\n📈 Step 900\n   Train loss: 7.2151\n   Val loss: 7.2059\n   Learning rate: 0.000025\n   💾 New best model saved! (val_loss: 7.2059)\n\n📈 Step 1,000\n   Train loss: 7.0778\n   Val loss: 7.0779\n   Learning rate: 0.000027\n   💾 New best model saved! (val_loss: 7.0779)\n\n📈 Step 1,100\n   Train loss: 6.9985\n   Val loss: 6.9907\n   Learning rate: 0.000030\n   💾 New best model saved! (val_loss: 6.9907)\n\n📈 Step 1,200\n   Train loss: 6.8951\n   Val loss: 6.9023\n   Learning rate: 0.000032\n   💾 New best model saved! (val_loss: 6.9023)\n\n📈 Step 1,300\n   Train loss: 6.8224\n   Val loss: 6.8059\n   Learning rate: 0.000034\n   💾 New best model saved! (val_loss: 6.8059)\n\n📈 Step 1,400\n   Train loss: 6.7860\n   Val loss: 6.8039\n   Learning rate: 0.000036\n   💾 New best model saved! (val_loss: 6.8039)\n\n📈 Step 1,500\n   Train loss: 6.7448\n   Val loss: 6.7370\n   Learning rate: 0.000039\n   💾 New best model saved! (val_loss: 6.7370)\n\n📈 Step 1,600\n   Train loss: 6.6296\n   Val loss: 6.6506\n   Learning rate: 0.000041\n   💾 New best model saved! (val_loss: 6.6506)\n\n📈 Step 1,700\n   Train loss: 6.5965\n   Val loss: 6.5700\n   Learning rate: 0.000043\n   💾 New best model saved! (val_loss: 6.5700)\n\n📈 Step 1,800\n   Train loss: 6.5333\n   Val loss: 6.5480\n   Learning rate: 0.000045\n   💾 New best model saved! (val_loss: 6.5480)\n\n📈 Step 1,900\n   Train loss: 6.4929\n   Val loss: 6.4927\n   Learning rate: 0.000048\n   💾 New best model saved! (val_loss: 6.4927)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Step 2,000\n   Train loss: 6.4417\n   Val loss: 6.4097\n   Learning rate: 0.000050\n   💾 New best model saved! (val_loss: 6.4097)\n\n📈 Step 2,100\n   Train loss: 6.3994\n   Val loss: 6.3958\n   Learning rate: 0.000050\n   💾 New best model saved! (val_loss: 6.3958)\n\n📈 Step 2,200\n   Train loss: 6.3783\n   Val loss: 6.3804\n   Learning rate: 0.000050\n   💾 New best model saved! (val_loss: 6.3804)\n\n📈 Step 2,300\n   Train loss: 6.2847\n   Val loss: 6.2741\n   Learning rate: 0.000050\n   💾 New best model saved! (val_loss: 6.2741)\n\n📈 Step 2,400\n   Train loss: 6.2729\n   Val loss: 6.3049\n   Learning rate: 0.000050\n\n📈 Step 2,500\n   Train loss: 6.1999\n   Val loss: 6.2249\n   Learning rate: 0.000050\n   💾 New best model saved! (val_loss: 6.2249)\n\n📈 Step 2,600\n   Train loss: 6.2277\n   Val loss: 6.2473\n   Learning rate: 0.000050\n\n📈 Step 2,700\n   Train loss: 6.1681\n   Val loss: 6.1565\n   Learning rate: 0.000050\n   💾 New best model saved! (val_loss: 6.1565)\n\n📈 Step 2,800\n   Train loss: 6.1330\n   Val loss: 6.1219\n   Learning rate: 0.000050\n   💾 New best model saved! (val_loss: 6.1219)\n\n📈 Step 2,900\n   Train loss: 6.1303\n   Val loss: 6.1461\n   Learning rate: 0.000050\n\n📈 Step 3,000\n   Train loss: 6.0961\n   Val loss: 6.0870\n   Learning rate: 0.000049\n   💾 New best model saved! (val_loss: 6.0870)\n\n📈 Step 3,100\n   Train loss: 6.0481\n   Val loss: 6.0764\n   Learning rate: 0.000049\n   💾 New best model saved! (val_loss: 6.0764)\n\n📈 Step 3,200\n   Train loss: 6.0594\n   Val loss: 6.0537\n   Learning rate: 0.000049\n   💾 New best model saved! (val_loss: 6.0537)\n\n📈 Step 3,300\n   Train loss: 6.0480\n   Val loss: 6.0496\n   Learning rate: 0.000049\n   💾 New best model saved! (val_loss: 6.0496)\n\n📈 Step 3,400\n   Train loss: 6.0116\n   Val loss: 6.0295\n   Learning rate: 0.000049\n   💾 New best model saved! (val_loss: 6.0295)\n\n📈 Step 3,500\n   Train loss: 6.0327\n   Val loss: 6.0154\n   Learning rate: 0.000049\n   💾 New best model saved! (val_loss: 6.0154)\n\n📈 Step 3,600\n   Train loss: 6.0374\n   Val loss: 6.0330\n   Learning rate: 0.000049\n\n📈 Step 3,700\n   Train loss: 6.0082\n   Val loss: 6.0283\n   Learning rate: 0.000049\n\n📈 Step 3,800\n   Train loss: 6.0013\n   Val loss: 6.0048\n   Learning rate: 0.000048\n   💾 New best model saved! (val_loss: 6.0048)\n\n📈 Step 3,900\n   Train loss: 5.9964\n   Val loss: 6.0018\n   Learning rate: 0.000048\n   💾 New best model saved! (val_loss: 6.0018)\n\n================================================================================\nPHASE 2 COMPLETE!\n================================================================================\n\n📊 Final Results:\n   Best validation loss: 6.0018\n   Final train loss: 5.9897\n   Final val loss: 6.0262\n   Model saved to: ./best_financial_slm.pt\n\n🎯 Two-Phase Training Complete!\n   Phase 1: English structure learned\n   Phase 2: Financial knowledge added\n   Result: Model understands English AND finance\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Step 12: Plot Training Loss Curves","metadata":{"id":"wiF9wwd87Nhf"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"VISUALIZING TRAINING RESULTS\")\nprint(\"=\" * 80)\n\n# Convert losses to CPU for plotting\ntrain_losses_cpu = [loss.cpu() if torch.is_tensor(loss) else loss for loss in train_losses]\nval_losses_cpu = [loss.cpu() if torch.is_tensor(loss) else loss for loss in val_losses]\n\n# Create loss plot\nplt.figure(figsize=(12, 6))\nplt.plot(steps_list, train_losses_cpu, 'g-', label='Train Loss', linewidth=2)\nplt.plot(steps_list, val_losses_cpu, 'r-', label='Validation Loss', linewidth=2)\nplt.xlabel('Training Steps', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.title('Financial SLM Training Progress', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('training_loss.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\n✓ Training curve saved as 'training_loss.png'\")\n\n# Print loss statistics\nprint(f\"\\n📊 Loss Statistics:\")\nprint(f\"   Initial train loss: {train_losses_cpu[0]:.4f}\")\nprint(f\"   Initial val loss: {val_losses_cpu[0]:.4f}\")\nprint(f\"   Final train loss: {train_losses_cpu[-1]:.4f}\")\nprint(f\"   Final val loss: {val_losses_cpu[-1]:.4f}\")\nprint(f\"   Best val loss: {best_val_loss:.4f}\")\nprint(f\"   Total improvement: {val_losses_cpu[0] - best_val_loss:.4f} ({(val_losses_cpu[0] - best_val_loss)/val_losses_cpu[0]*100:.1f}%)\")\n\nprint(\"\\n\" + \"=\" * 80)","metadata":{"id":"eg93z01b5eoY","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T15:40:17.274550Z","iopub.execute_input":"2025-10-30T15:40:17.274850Z","iopub.status.idle":"2025-10-30T15:40:17.846295Z","shell.execute_reply.started":"2025-10-30T15:40:17.274825Z","shell.execute_reply":"2025-10-30T15:40:17.845627Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nVISUALIZING TRAINING RESULTS\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\n✓ Training curve saved as 'training_loss.png'\n\n📊 Loss Statistics:\n   Initial train loss: 12.0570\n   Initial val loss: 12.0968\n   Final train loss: 5.9964\n   Final val loss: 6.0018\n   Best val loss: 6.0018\n   Total improvement: 6.0950 (50.4%)\n\n================================================================================\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Step 13: Model Summary","metadata":{}},{"cell_type":"code","source":"# Create Model Card & Save Training Summary\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CREATING MODEL DOCUMENTATION\")\nprint(\"=\" * 80)\n\nconfig = model.module.config if is_multi_gpu else model.config \n# Create model card\nmodel_card = f\"\"\"\n# Financial SLM - Model Card\n\n## Model Details\n- **Model Name**: Financial Small Language Model (SLM)\n- **Model Type**: GPT-2 Fine-tuned Transformer\n- **Parameters**: {n_params/1e6:.2f}M\n- **Architecture**: {config.n_layer} layers, {config.n_head} attention heads, {config.n_embd} embedding dim\n- **Vocabulary**: GPT-2 (50,257 tokens)\n- **Training Date**: October 29, 2025\n\n## Training Data\n- **Finance Alpaca**: {len(finance_alpaca['train']):,} financial Q&A examples\n- **Financial PhraseBank**: {len(financial_phrasebank['train']):,} sentiment analysis examples\n- **Custom Investment Data**: {len(custom_dataset)} curated investment examples\n- **Total Training Examples**: {len(dataset_split['train']):,}\n- **Total Tokens**: {total_tokens:,}\n\n## Training Configuration\n- **Optimizer**: AdamW (lr={learning_rate}, weight_decay=0.1)\n- **Batch Size**: {batch_size} (effective: {batch_size * gradient_accumulation_steps})\n- **Training Steps**: {max_iters:,}\n- **Learning Rate Schedule**: Linear warmup ({warmup_steps} steps) + Cosine annealing\n- **Mixed Precision**: {dtype}\n- **Hardware**: {'GPU' if device_type == 'cuda' else 'CPU'}\n\n## Performance\n- **Best Validation Loss**: {best_val_loss:.4f}\n- **Evaluations Performed**: {len(train_losses)}\n\n## Capabilities\n- Financial question answering\n- Investment analysis and recommendations\n- Sentiment analysis of financial statements\n- Company financial health assessment\n- Portfolio diversification advice\n- Financial concept explanations\n\n## Limitations\n- Educational purposes only - not financial advice\n- May generate incorrect or outdated information\n- Should not be sole basis for investment decisions\n- Requires human verification and professional advice\n\n## Intended Use\n- Financial education and learning\n- Investment research assistance\n- Financial literacy improvement\n- Prototype for financial chatbots\n\n## Ethical Considerations\n- Always include disclaimers about not being financial advice\n- Users should consult licensed financial advisors\n- Model outputs should be fact-checked\n- Not suitable for automated trading systems\n\n## Citation\nIf you use this model, please cite:\n    Financial SLM (2025)\n    {n_params/1e6:.1f}M parameter GPT-2 fine-tuned model for financial analysis\n    Trained on Finance Alpaca + Financial PhraseBank + Custom data\n\"\"\"\n\n# Save model card to file\nwith open('model_card.md', 'w') as f:\n    f.write(model_card)\n\nprint(\"✓ Model card saved: model_card.md\")\n\n# Print training summary\nprint(\"\\n\" + \"=\" * 80)\nprint(\"📊 TRAINING SUMMARY\")\nprint(\"=\" * 80)\n\nprint(f\"\\n🔧 Model Configuration:\")\nprint(f\"  Total parameters: {n_params:,} ({n_params/1e6:.2f}M)\")\nprint(f\"  Layers: {config.n_layer}\")\nprint(f\"  Attention heads: {config.n_head}\")\nprint(f\"  Embedding dimension: {config.n_embd}\")\nprint(f\"  Block size: {block_size}\")\nprint(f\"  Vocabulary size: {config.vocab_size:,}\")\n\nprint(f\"\\n📚 Training Data:\")\nprint(f\"  Total training examples: {len(dataset_split['train']):,}\")\nprint(f\"  Total test examples: {len(dataset_split['test']):,}\")\nprint(f\"  Total tokens: {total_tokens:,}\")\nprint(f\"  Avg tokens per example: {total_tokens / len(combined_dataset):.1f}\")\n\nprint(f\"\\n⚙️ Training Configuration:\")\nprint(f\"  Learning rate: {learning_rate}\")\nprint(f\"  Training iterations: {max_iters:,}\")\nprint(f\"  Batch size: {batch_size}\")\nprint(f\"  Gradient accumulation: {gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {batch_size * gradient_accumulation_steps}\")\nprint(f\"  Warmup steps: {warmup_steps}\")\nprint(f\"  Eval interval: {eval_interval}\")\nprint(f\"  Mixed precision: {dtype}\")\n\nprint(f\"\\n📈 Training Performance:\")\nprint(f\"  Evaluations performed: {len(train_losses)}\")\nprint(f\"  Best validation loss: {best_val_loss:.4f}\")\n\nif len(train_losses) > 0:\n    initial_train = train_losses[0].item() if torch.is_tensor(train_losses[0]) else train_losses[0]\n    final_train = train_losses[-1].item() if torch.is_tensor(train_losses[-1]) else train_losses[-1]\n    initial_val = val_losses[0].item() if torch.is_tensor(val_losses[0]) else val_losses[0]\n    final_val = val_losses[-1].item() if torch.is_tensor(val_losses[-1]) else val_losses[-1]\n    \n    print(f\"  Initial train loss: {initial_train:.4f}\")\n    print(f\"  Final train loss: {final_train:.4f}\")\n    print(f\"  Initial val loss: {initial_val:.4f}\")\n    print(f\"  Final val loss: {final_val:.4f}\")\n    \n    improvement = initial_val - best_val_loss\n    improvement_pct = (improvement / initial_val * 100) if initial_val > 0 else 0\n    print(f\"  Total improvement: {improvement:.4f} ({improvement_pct:.1f}%)\")\n\nprint(f\"\\n💾 Files Created:\")\nprint(f\"  ✓ best_financial_slm.pt - Model checkpoint\")\nprint(f\"  ✓ training_loss.png - Loss curve visualization\")\nprint(f\"  ✓ MODEL_CARD.md - Model documentation\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"🎉 ALL STEPS COMPLETE - YOUR FINANCIAL SLM IS READY!\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T15:40:17.847086Z","iopub.execute_input":"2025-10-30T15:40:17.847369Z","iopub.status.idle":"2025-10-30T15:40:17.860699Z","shell.execute_reply.started":"2025-10-30T15:40:17.847342Z","shell.execute_reply":"2025-10-30T15:40:17.860082Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCREATING MODEL DOCUMENTATION\n================================================================================\n✓ Model card saved: model_card.md\n\n================================================================================\n📊 TRAINING SUMMARY\n================================================================================\n\n🔧 Model Configuration:\n  Total parameters: 109,674,240 (109.67M)\n  Layers: 10\n  Attention heads: 12\n  Embedding dimension: 768\n  Block size: 256\n  Vocabulary size: 50,257\n\n📚 Training Data:\n  Total training examples: 82,900\n  Total test examples: 9,212\n  Total tokens: 11,490,898\n  Avg tokens per example: 124.7\n\n⚙️ Training Configuration:\n  Learning rate: 5e-05\n  Training iterations: 4,000\n  Batch size: 8\n  Gradient accumulation: 4\n  Effective batch size: 32\n  Warmup steps: 500\n  Eval interval: 100\n  Mixed precision: bfloat16\n\n📈 Training Performance:\n  Evaluations performed: 40\n  Best validation loss: 6.0018\n  Initial train loss: 12.0570\n  Final train loss: 5.9964\n  Initial val loss: 12.0968\n  Final val loss: 6.0018\n  Total improvement: 6.0950 (50.4%)\n\n💾 Files Created:\n  ✓ best_financial_slm.pt - Model checkpoint\n  ✓ training_loss.png - Loss curve visualization\n  ✓ MODEL_CARD.md - Model documentation\n\n================================================================================\n🎉 ALL STEPS COMPLETE - YOUR FINANCIAL SLM IS READY!\n================================================================================\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Step 14: Model Inference and Testing","metadata":{}},{"cell_type":"code","source":"import tiktoken\nimport torch\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LOADING BEST MODEL FOR INFERENCE\")\nprint(\"=\" * 80)\n\nbest_model_path = 'best_financial_slm.pt'\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice_type = 'cuda' if 'cuda' in device else 'cpu'\n\n# Load the best checkpoint\ncheckpoint = torch.load(best_model_path, map_location=device, weights_only=False)\nprint(f\"\\n📂 Loading checkpoint from: {best_model_path}\")\n\n# Create model with saved config\ninference_model = GPT(checkpoint['config']).to(device)\ninference_model.load_state_dict(checkpoint['model_state_dict'])\ninference_model.eval()\n\nprint(f\"✓ Model loaded successfully\")\nprint(f\"  Training iteration: {checkpoint['iter_num']:,}\")\nprint(f\"  Validation loss: {checkpoint['val_loss']:.4f}\")\n\n# Initialize tokenizer for inference\nenc = tiktoken.get_encoding(\"gpt2\")\nprint(f\"\\n✓ Tokenizer ready (vocab size: {enc.n_vocab:,})\")\n\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T15:40:17.861543Z","iopub.execute_input":"2025-10-30T15:40:17.862001Z","iopub.status.idle":"2025-10-30T15:40:21.395682Z","shell.execute_reply.started":"2025-10-30T15:40:17.861977Z","shell.execute_reply":"2025-10-30T15:40:21.395065Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nLOADING BEST MODEL FOR INFERENCE\n================================================================================\n\n📂 Loading checkpoint from: best_financial_slm.pt\n✓ Model loaded successfully\n  Training iteration: 3,900\n  Validation loss: 6.0018\n\n✓ Tokenizer ready (vocab size: 50,257)\n================================================================================\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def generate_financial_response(prompt, max_tokens=200, temperature=0.7, top_k=50):\n    \"\"\"\n    Generate a financial response from the trained model.\n    \n    Args:\n        prompt: The input prompt (instruction/question)\n        max_tokens: Maximum tokens to generate\n        temperature: Sampling temperature (0.7 = balanced, 1.0 = creative, 0.5 = focused)\n        top_k: Top-k sampling (limits to k most likely tokens)\n    \n    Returns:\n        Generated text response\n    \"\"\"\n    # Format prompt with financial template\n    formatted_prompt = f\"Instruction: {prompt}\\nInput: \\nResponse: \"\n    \n    context = torch.tensor(enc.encode_ordinary(formatted_prompt), \n                          dtype=torch.long, device=device).unsqueeze(0)\n\n    inference_model.eval()\n    \n    with torch.no_grad():\n        # Remove topk to get more diverse output from undertrained model\n        output = inference_model.generate(context, max_new_tokens=max_tokens, \n                               temperature=temperature)\n    \n    full_response = enc.decode(output[0].tolist())\n    \n    # Better response extraction\n    if \"Response:\" in full_response:\n        response = full_response.split(\"Response:\")[-1]\n        # Stop at \"Instruction\" or \"End\"\n        for stop_word in [\"\\nInstruction\", \"End\", \"\\n\\n\\n\"]:\n            if stop_word in response:\n                response = response.split(stop_word)[0]\n        return response.strip()\n    \n    return full_response\n\nprint(\"✓ Inference function defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T15:40:21.396400Z","iopub.execute_input":"2025-10-30T15:40:21.396605Z","iopub.status.idle":"2025-10-30T15:40:21.403719Z","shell.execute_reply.started":"2025-10-30T15:40:21.396581Z","shell.execute_reply":"2025-10-30T15:40:21.403005Z"}},"outputs":[{"name":"stdout","text":"✓ Inference function defined\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 80)\nprint(\"🤖 FINANCIAL SLM - INFERENCE DEMO\")\nprint(\"=\" * 80)\n\n# Define test queries covering different financial topics\ntest_queries = [\n    {\n        \"category\": \"Investment Analysis\",\n        \"query\": \"Should I invest in renewable energy stocks?\"\n    },\n    {\n        \"category\": \"Financial Metrics\",\n        \"query\": \"What is a good debt-to-equity ratio for tech companies?\"\n    },\n    {\n        \"category\": \"Portfolio Management\",\n        \"query\": \"How should I rebalance my portfolio during a market downturn?\"\n    },\n    {\n        \"category\": \"Company Analysis\",\n        \"query\": \"What financial metrics indicate a company is undervalued?\"\n    },\n    {\n        \"category\": \"Risk Assessment\",\n        \"query\": \"What are the risks of investing in emerging markets?\"\n    }\n]\n\n# Run inference on each query\nfor i, test in enumerate(test_queries, 1):\n    print(f\"\\n{'─'*80}\")\n    print(f\"📊 Test {i}/{len(test_queries)}: {test['category']}\")\n    print(f\"{'─'*80}\")\n    print(f\"\\n❓ Query: {test['query']}\")\n    print(f\"\\n💡 Response:\")\n    \n    response = generate_financial_response(\n        test['query'],\n        max_tokens=200,\n        temperature=0.7,\n        top_k=None\n    )\n    \n    print(response)\n\nprint(f\"\\n{'='*80}\")\nprint(\"✅ INFERENCE DEMO COMPLETE\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T15:40:21.404590Z","iopub.execute_input":"2025-10-30T15:40:21.404831Z","iopub.status.idle":"2025-10-30T15:40:30.289958Z","shell.execute_reply.started":"2025-10-30T15:40:21.404811Z","shell.execute_reply":"2025-10-30T15:40:30.289290Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\n🤖 FINANCIAL SLM - INFERENCE DEMO\n================================================================================\n\n────────────────────────────────────────────────────────────────────────────────\n📊 Test 1/5: Investment Analysis\n────────────────────────────────────────────────────────────────────────────────\n\n❓ Query: Should I invest in renewable energy stocks?\n\n💡 Response:\nThe a list of the time to life, and the variety of your total lot of the particular.\n\n────────────────────────────────────────────────────────────────────────────────\n📊 Test 2/5: Financial Metrics\n────────────────────────────────────────────────────────────────────────────────\n\n❓ Query: What is a good debt-to-equity ratio for tech companies?\n\n💡 Response:\nI was a days, for a short-term, and living in July those.\n\n###\n\n────────────────────────────────────────────────────────────────────────────────\n📊 Test 3/5: Portfolio Management\n────────────────────────────────────────────────────────────────────────────────\n\n❓ Query: How should I rebalance my portfolio during a market downturn?\n\n💡 Response:\n- This maximum ratings, I can be the economy, and the to one systems. \n- If you can also be party to make positions better constantly theiveages. \n- How be found that is it that theened to a sentence. \n- The inspiration to prevent this\" is is also you should beav.\n- What are used to their bad parts of the importance of an crash and the most price and long and buildings.\n- The monitor up to create a styles.\n- speak and information and the box instead of their owner.\n- place to no process.\n- At the used to an the computer, the list of of the sh. \n- The process of a service is the stock and not the revolution.\n- The i\n- The same company is this is a sentence. \n- The, and an distance of the news of a order for a distance of a resources. \n-0. \n-\n\n────────────────────────────────────────────────────────────────────────────────\n📊 Test 4/5: Company Analysis\n────────────────────────────────────────────────────────────────────────────────\n\n❓ Query: What financial metrics indicate a company is undervalued?\n\n💡 Response:\n- How I have to do stock:\n- 10- 3: \n- 1: \n- 2: \n- 2: \n- 1\n- 2: \n- 1\n- 2\n- 4: \n- 2:\n- 2: \n- 2: \n- 2:\n- 1\n- 1).\n- A:\n- 2: \n- 2 \n- 1:\n- 1 \n- 2\n- 2, 0\n- 0\n- 1 myery\n- 2:\n- 1, 4, 6, 50\n- 2\n- 1\n- 4:\n- 5\n- 6, 2: \n- 0 you don't a money.\n- 1, 1\n- 0\n- 2: I can animal with a lot of the afford.\n- A- interest.\n- 5?\n- \"I costsage\n- 2:\n- 2:\n\n────────────────────────────────────────────────────────────────────────────────\n📊 Test 5/5: Risk Assessment\n────────────────────────────────────────────────────────────────────────────────\n\n❓ Query: What are the risks of investing in emerging markets?\n\n💡 Response:\nagree for a: \n- The matter in the Or unlike a\n- The biggest could have to the same case of stated.\n- Natural. \n- Well \n- The risk of the following sentiment of damage to make a AI\n- The country of the following certainly acquired I would be used to understand. \n- From the good, you. \n- The tasks is a new stand to a sentence in the eye and the amount of the given res.\n- The value of a common to the world.\n- What could have a \n- How weekra and trade.\n- and Human demand of the C- What would have a word by the a end of theie.\n-t the following term clear in the following releases, the given sentence is images.\n- It is a copies to be used to those returns.\n- The list of the On the two acts of the most of the following money.\n- The candidate\n\n================================================================================\n✅ INFERENCE DEMO COMPLETE\n================================================================================\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Step 15: Upload to huggingfaces as a new version of the model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import HfApi\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\n# Get token from Kaggle secrets\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\napi = HfApi(token=hf_token)\n\n# Upload the folder\napi.upload_folder(\n    folder_path=\"/kaggle/working/\",\n    repo_id=\"satgun/finpeak-ai\",\n    commit_message=\"Upload finpeak-ai model\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T16:56:11.985103Z","iopub.execute_input":"2025-10-30T16:56:11.985403Z","iopub.status.idle":"2025-10-30T16:56:42.585124Z","shell.execute_reply.started":"2025-10-30T16:56:11.985378Z","shell.execute_reply":"2025-10-30T16:56:42.584214Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd7d7dbf847c473b91fbc6e462e98166"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ee49a563a394d4e9c67b7d7a921ce27"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Satgun/finpeak-ai/commit/296ffec00ce307318e26ad84c77575129ff00345', commit_message='Upload finpeak-ai model', commit_description='', oid='296ffec00ce307318e26ad84c77575129ff00345', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Satgun/finpeak-ai', endpoint='https://huggingface.co', repo_type='model', repo_id='Satgun/finpeak-ai'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":5}]}